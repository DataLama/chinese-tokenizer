{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hanlp\n",
    "print(hanlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://file.hankcs.com/hanlp/cws/large_cws_albert_base_20200828_011451.zip to /root/.hanlp/cws/large_cws_albert_base_20200828_011451.zip\n",
      "65.26%, 23.4 MB/35.9 MB, 9.1 MB/s, ETA 1 s      IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.00%, 35.9 MB/35.9 MB, 9.4 MB/s, ETA 0 s      \n",
      "Extracting /root/.hanlp/cws/large_cws_albert_base_20200828_011451.zip to /root/.hanlp/cws\n",
      "Downloading https://file.hankcs.com/hanlp/embeddings/albert_base_zh.tar.gz to /root/.hanlp/embeddings/albert_base_zh.tar.gz\n",
      "61.85%, 23.4 MB/37.9 MB, 9.3 MB/s, ETA 2 s      IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.00%, 37.9 MB/37.9 MB, 9.8 MB/s, ETA 0 s      \n",
      "Extracting /root/.hanlp/embeddings/albert_base_zh.tar.gz to /root/.hanlp/embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = hanlp.load(hanlp.pretrained.cws.LARGE_ALBERT_BASE)\n",
    "tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://file.hankcs.com/hanlp/ner/ner_albert_base_zh_msra_20200111_202919.zip to /root/.hanlp/ner/ner_albert_base_zh_msra_20200111_202919.zip\n",
      "66.14%, 23.5 MB/35.5 MB, 8.4 MB/s, ETA 1 s      IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.00%, 35.5 MB/35.5 MB, 9.0 MB/s, ETA 0 s      \n",
      "Extracting /root/.hanlp/ner/ner_albert_base_zh_msra_20200111_202919.zip to /root/.hanlp/ner\n"
     ]
    }
   ],
   "source": [
    "ner_tagger = hanlp.load('MSRA_NER_ALBERT_BASE_ZH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = hanlp.pipeline() \\\n",
    "    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \\\n",
    "    .append(tokenizer, output_key='tokens') \\\n",
    "    .append(tagger, output_key='part_of_speech_tags') \\\n",
    "    .append(ner_tagger, output_key='ner_tags')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_pos = {'AD', 'AS', 'BA', 'CC', 'CS','DEC','DEG','DER','DEV','DT', 'ETC',  'LB', 'LC', 'MSP', 'NT', 'OD','ON', 'P','PN','PU','SB','SP','VA','VC','VE','NP','SP'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CD', 'FW', 'IJ', 'JJ', 'M', 'NN', 'NR', 'VP', 'VV', 'X'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tagger.tag_vocab.tokens).difference(useless_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IJ는 ㅋㅋㅋㅋ 같은거임.\n",
    "- 'JJ' + 'NN' JJ는 NN이랑 같이 나올때 의미가 있음.\n",
    "- OD는 서수형\n",
    "- ON은 의성어 (우는게 많다고함.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use_less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['NR', 'NN', 'CC', 'VV', 'NT', 'PU', 'LC', 'AS', 'ETC', 'DEC', 'CD', 'M', 'DEG', 'JJ', 'VC', 'AD', 'P', 'PN', 'VA', 'DEV', 'DT', 'SB', 'OD', 'VE', 'CS', 'MSP', 'BA', 'FW', 'LB', 'DER', 'SP', 'NP', 'IJ', 'X', 'VP'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_vocab.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': ['雪花霜我是回购五六瓶了吧，活动时候黑金卡特惠专用的太难抢了，我从来就没抢到货，到了九点马上付款就没了，只好退而求其次抢了这个套装。'],\n",
       " 'tokens': [['雪花霜',\n",
       "   '我',\n",
       "   '是',\n",
       "   '回购',\n",
       "   '五六',\n",
       "   '瓶',\n",
       "   '了',\n",
       "   '吧',\n",
       "   '，',\n",
       "   '活动',\n",
       "   '时候',\n",
       "   '黑金卡',\n",
       "   '特惠',\n",
       "   '专用',\n",
       "   '的',\n",
       "   '太',\n",
       "   '难',\n",
       "   '抢',\n",
       "   '了',\n",
       "   '，',\n",
       "   '我',\n",
       "   '从来',\n",
       "   '就',\n",
       "   '没',\n",
       "   '抢到',\n",
       "   '货',\n",
       "   '，',\n",
       "   '到',\n",
       "   '了',\n",
       "   '九点',\n",
       "   '马上',\n",
       "   '付款',\n",
       "   '就',\n",
       "   '没',\n",
       "   '了',\n",
       "   '，',\n",
       "   '只好',\n",
       "   '退而求其次',\n",
       "   '抢',\n",
       "   '了',\n",
       "   '这',\n",
       "   '个',\n",
       "   '套装',\n",
       "   '。']],\n",
       " 'part_of_speech_tags': [['NN',\n",
       "   'PN',\n",
       "   'VC',\n",
       "   'VV',\n",
       "   'CD',\n",
       "   'M',\n",
       "   'AS',\n",
       "   'SP',\n",
       "   'PU',\n",
       "   'NN',\n",
       "   'P',\n",
       "   'NR',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   'DEC',\n",
       "   'AD',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'AS',\n",
       "   'PU',\n",
       "   'PN',\n",
       "   'AD',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'VV',\n",
       "   'NN',\n",
       "   'PU',\n",
       "   'VV',\n",
       "   'AS',\n",
       "   'NT',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'AS',\n",
       "   'PU',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'VV',\n",
       "   'AS',\n",
       "   'DT',\n",
       "   'M',\n",
       "   'NN',\n",
       "   'PU']],\n",
       " 'ner_tags': [[('NN PN VC VV', 'NT', 0, 4),\n",
       "   ('AD', 'NT', 15, 16),\n",
       "   ('VV', 'NT', 17, 18),\n",
       "   ('PU', 'NT', 19, 20),\n",
       "   ('PN', 'NR', 20, 21),\n",
       "   ('AD', 'NT', 21, 22),\n",
       "   ('NN', 'NR', 25, 26),\n",
       "   ('AS', 'NT', 28, 29)]],\n",
       " '_ipython_canary_method_should_not_exist_': []}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"\"\"雪花霜我是回购五六瓶了吧，活动时候黑金卡特惠专用的太难抢了，我从来就没抢到货，到了九点马上付款就没了，只好退而求其次抢了这个套装。\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_path': 'hanlp.components.ner.TransformerNamedEntityRecognizer',\n",
       " 'hanlp_version': '2.0.0-alpha.23',\n",
       " 'load_path': 'https://file.hankcs.com/hanlp/ner/ner_albert_base_zh_msra_20200111_202919.zip'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_capture_config',\n",
       " 'build',\n",
       " 'build_callbacks',\n",
       " 'build_loss',\n",
       " 'build_metrics',\n",
       " 'build_model',\n",
       " 'build_optimizer',\n",
       " 'build_train_dataset',\n",
       " 'build_transform',\n",
       " 'build_valid_dataset',\n",
       " 'build_vocab',\n",
       " 'compile_model',\n",
       " 'config',\n",
       " 'evaluate',\n",
       " 'evaluate_dataset',\n",
       " 'evaluate_output',\n",
       " 'evaluate_output_to_file',\n",
       " 'export_model_for_serving',\n",
       " 'fit',\n",
       " 'from_meta',\n",
       " 'input_shape',\n",
       " 'load',\n",
       " 'load_config',\n",
       " 'load_meta',\n",
       " 'load_transform',\n",
       " 'load_vocabs',\n",
       " 'load_weights',\n",
       " 'meta',\n",
       " 'model',\n",
       " 'on_train_begin',\n",
       " 'predict',\n",
       " 'predict_batch',\n",
       " 'sample_data',\n",
       " 'save',\n",
       " 'save_config',\n",
       " 'save_meta',\n",
       " 'save_vocabs',\n",
       " 'save_weights',\n",
       " 'serve',\n",
       " 'train_loop',\n",
       " 'transform']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ner_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_eagerly': True,\n",
       " 'epochs': 3,\n",
       " 'batch_size': 32,\n",
       " 'metrics': 'f1',\n",
       " 'max_seq_length': 128,\n",
       " 'use_amp': False,\n",
       " 'warmup_steps_ratio': 0,\n",
       " 'clipnorm': 1.0,\n",
       " 'epsilon': 1e-08,\n",
       " 'weight_decay_rate': 0,\n",
       " 'learning_rate': 5e-05,\n",
       " 'optimizer': {'class_name': 'AdamWeightDecay',\n",
       "  'config': {'name': 'AdamWeightDecay',\n",
       "   'clipnorm': 1.0,\n",
       "   'learning_rate': {'class_name': 'PolynomialDecay',\n",
       "    'config': {'initial_learning_rate': 5e-05,\n",
       "     'decay_steps': 3990,\n",
       "     'end_learning_rate': 0.0,\n",
       "     'power': 1.0,\n",
       "     'cycle': False,\n",
       "     'name': None}},\n",
       "   'decay': 0.0,\n",
       "   'beta_1': 0.9,\n",
       "   'beta_2': 0.999,\n",
       "   'epsilon': 1e-08,\n",
       "   'amsgrad': False,\n",
       "   'weight_decay_rate': 0}},\n",
       " 'transformer': 'albert_base_zh',\n",
       " 'warmup_steps': 0,\n",
       " 'train_steps': 3990,\n",
       " 'training': False,\n",
       " 'logger': <Logger hanlp (INFO)>}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d652229ef75f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhanlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'既然句法和语义分析依赖于词性标注'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "hanlp.common.constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = hanlp.load('CTB6_CONVSEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baidu lac "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAC import LAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lac = LAC(mode='lac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_pos= {'f', 'nw', 'vd', 'ad', 'd', 'r', 'p', 'c', 'u', 'xc', 'w'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"SK-II 晶透经典礼盒\n",
    "无限回购的神仙水，混油皮很适合，冬天用感觉稍有点干。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SK-II', 'nz')\n",
      "(' ', 'w')\n",
      "('晶透经典礼盒\\n', 'nz')\n",
      "('无限', 'ad')\n",
      "('回购', 'v')\n",
      "('的', 'u')\n",
      "('神仙水', 'n')\n",
      "('，', 'w')\n",
      "('混', 'v')\n",
      "('油皮', 'n')\n",
      "('很', 'd')\n",
      "('适合', 'v')\n",
      "('，', 'w')\n",
      "('冬天', 'TIME')\n",
      "('用', 'p')\n",
      "('感觉', 'n')\n",
      "('稍', 'd')\n",
      "('有点', 'd')\n",
      "('干', 'a')\n",
      "('。', 'w')\n"
     ]
    }
   ],
   "source": [
    "for token in zip(*lac.run(text)):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
