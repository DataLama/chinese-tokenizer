{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "- NFKC()\n",
    "- 쓸모없는애들날리기..\n",
    "\n",
    "## Pre\n",
    "\n",
    "\n",
    "### hanlp\n",
    "- ??\n",
    "\n",
    "### lac\n",
    "- \n",
    "\n",
    "### bertkpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = hanlp.load(hanlp.pretrained.cws.LARGE_ALBERT_BASE)\n",
    "tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)\n",
    "ner_tagger = hanlp.load('MSRA_NER_ALBERT_BASE_ZH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = hanlp.pipeline() \\\n",
    "    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \\\n",
    "    .append(tokenizer, output_key='tokens') \\\n",
    "    .append(tagger, output_key='part_of_speech_tags') \\\n",
    "    .append(ner_tagger, output_key='ner_tags')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "with open('sample.txt') as f:\n",
    "    docs = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pipeline(normalize(\"兰蔻新清滢柔肤水(粉水)400ml惠选套组 这次买的，没上次买的好，无论瓶子，瓶盖，之类的都比上次的差，粉水也没上次浓，香\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['兰蔻新清滢柔肤水(粉水)400ml惠选套组 这次买的,没上次买的好,无论瓶子,瓶盖,之类的都比上次的差,粉水也没上次浓,香']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "兰蔻新 NR\n",
      "清滢 NR\n",
      "柔肤水 NN\n",
      "( PU\n",
      "粉水 NN\n",
      ") PU\n",
      "400 PU\n",
      "ml NN\n",
      "惠选 NN\n",
      "套组 NN\n",
      "  NR\n",
      "这次 AD\n",
      "买 VV\n",
      "的 DEC\n",
      ", NN\n",
      "没 VE\n",
      "上次 NT\n",
      "买 VV\n",
      "的 DEC\n",
      "好 JJ\n",
      ", PU\n",
      "无论 JJ\n",
      "瓶子 NN\n",
      ", VV\n",
      "瓶盖 NN\n",
      ", PU\n",
      "之类 VV\n",
      "的 DEC\n",
      "都 AD\n",
      "比 P\n",
      "上次 NT\n",
      "的 DEG\n",
      "差 JJ\n",
      ", NN\n",
      "粉水 NN\n",
      "也 AD\n",
      "没 VE\n",
      "上次 NT\n",
      "浓 VV\n",
      ", NN\n",
      "香 NN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tok, p in zip(chain(*tmp['tokens']), chain(*tmp['part_of_speech_tags'])):\n",
    "    print(tok, p )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NR',\n",
       "  'NR',\n",
       "  'NN',\n",
       "  'PU',\n",
       "  'NN',\n",
       "  'PU',\n",
       "  'PU',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  'NR',\n",
       "  'AD',\n",
       "  'VV',\n",
       "  'DEC',\n",
       "  'NN',\n",
       "  'VE',\n",
       "  'NT',\n",
       "  'VV',\n",
       "  'DEC',\n",
       "  'JJ',\n",
       "  'PU',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'VV',\n",
       "  'NN',\n",
       "  'PU',\n",
       "  'VV',\n",
       "  'DEC',\n",
       "  'AD',\n",
       "  'P',\n",
       "  'NT',\n",
       "  'DEG',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  'AD',\n",
       "  'VE',\n",
       "  'NT',\n",
       "  'VV',\n",
       "  'NN',\n",
       "  'NN']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['part_of_speech_tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VV + DEC => 합치면됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')400ml惠选'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(tmp['tokens'][0][5:9]) # 11 <= x < 12 # NR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'买的,没上次买的好,无论瓶子,瓶盖,之类的都'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(tmp['tokens'][0][12:29]) # NR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这次'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(tmp['tokens'][0][11:12]) # 11 <= x < 12 # NT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'兰蔻新清滢柔肤水(粉水'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(tmp['tokens'][0][0:5]) # NT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NR NR NN PU NN', 'NT', 0, 5),\n",
       "  ('PU PU NN NN', 'NR', 5, 9),\n",
       "  ('AD', 'NT', 11, 12),\n",
       "  ('VV DEC NN VE NT VV DEC JJ PU JJ NN VV NN PU VV DEC AD', 'NR', 12, 29),\n",
       "  ('NT', 'NR', 30, 31),\n",
       "  ('NN NN AD VE NT VV', 'NR', 33, 39),\n",
       "  ('NN', 'NR', 40, 41)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set = {'CD', 'DEC','FW', 'IJ', 'JJ', 'M', 'NN', 'NR', 'VP', 'VV', 'X'}\n",
    "# VV + DEC => join 하게 (DEC)-> 품사가 동명사\n",
    "# SP..\n",
    "# 이거 저거 DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "兰蔻新 NR\n",
      "清滢 NR\n",
      "柔肤水 NN\n",
      "( PU\n",
      "粉水 NN\n",
      ") PU\n",
      "400 PU\n",
      "ml NN\n",
      "惠选 NN\n",
      "套组 NN\n",
      "  NR\n",
      "这次 AD\n",
      "买 VV\n",
      "的 DEC\n",
      ", NN\n",
      "没 VE\n",
      "上次 NT\n",
      "买 VV\n",
      "的 DEC\n",
      "好 JJ\n",
      ", PU\n",
      "无论 JJ\n",
      "瓶子 NN\n",
      ", VV\n",
      "瓶盖 NN\n",
      ", PU\n",
      "之类 VV\n",
      "的 DEC\n",
      "都 AD\n",
      "比 P\n",
      "上次 NT\n",
      "的 DEG\n",
      "差 JJ\n",
      ", NN\n",
      "粉水 NN\n",
      "也 AD\n",
      "没 VE\n",
      "上次 NT\n",
      "浓 VV\n",
      ", NN\n",
      "香 NN\n"
     ]
    }
   ],
   "source": [
    "doc = '兰蔻新清滢柔肤水(粉水)400ml惠选套组 这次买的，没上次买的好，无论瓶子，瓶盖，之类的都比上次的差，粉水也没上次浓，香'\n",
    "result = pipeline(normalize(doc))\n",
    "for tok, p in zip(chain(*result['tokens']), chain(*result['part_of_speech_tags'])):\n",
    "\n",
    "    print(tok, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20-10-06 16:28:05 WARNING Input tokens ['雅', '诗', '兰', '黛', '特', '润', '密', '集', '修', '护', '浓', '缩', '精', '华', '素', '惠', '选', '套', '组', ' ', ' ', '虽', '说', '是', '赠', '品', '吧', ',', '但', '是', '都', '漏', '了', ',', '也', '太', '敷', '衍', '了', '吧', ',', '高', '能', '小', '棕', '瓶', '挺', '好', '用', '的', ',', '赶', '上', '打', '折', '囤', '货', ',', '结', '果', '回', '来', '的', '赠', '品', '精', '华', '小', '包', '装', '盒', '上', '都', '变', '色', '了', ',', '打', '开', '一', '看', ',', '瓶', '子', '上', '都', '是', '干', '了', '的', '黄', '色', '液', '体', ',', '拧', '开', '一', '看', '瓶', '口', '都', '是', ',', '哎', ',', '也', '不', '知', '道', '怎', '么', '发', '图', '片', ',', '之', '前', '都', '没', '写', '过', '评', '价', ',', '找', '了', '半', '天', '才', '找', '到'] exceed the max sequence length of 126. The exceeded part will be truncated and ignored. You are recommended to split your long text into several sentences within 126 tokens beforehand.\n",
      "20-10-06 16:28:16 WARNING Input tokens ['单', '日', '销', '售', '破', '3', '5', '4', '1', '万', ',', '成', '都', '大', '悦', '城', '逆', '市', '引', '爆', '疯', '抢', '狂', '欢', ' ', ' ', '成', '都', '2', '0', '2', '0', '年', '9', '月', '1', '4', '日', ' ', '/', '美', '通', '社', '/', ' ', '-', '-', ' ', '刚', '刚', '过', '去', '的', '周', '末', ',', '一', '年', '一', '度', '的', ' ', '\"', ' ', '大', '悦', '疯', '抢', '节', '\"', ' ', '又', '一', '次', '刷', '爆', '朋', '友', '圈', ',', '活', '动', '当', '天', ',', '成', '都', '大', '悦', '城', '收', '获', '了', ' ', '3', '5', '4', '1', '万', '元', ' ', '的', '销', '售', '业', '绩', ',', '以', '及', ' ', '1', '1', '.', '1', '万', '人', '次', ' ', '的', '客', '流', '量', ',', '总', '车', '流', '突', '破'] exceed the max sequence length of 126. The exceeded part will be truncated and ignored. You are recommended to split your long text into several sentences within 126 tokens beforehand.\n"
     ]
    }
   ],
   "source": [
    "my_tok = []\n",
    "\n",
    "for doc in docs:\n",
    "#     doc = normalize(doc)\n",
    "    inferenced = pipeline(doc)\n",
    "    tokens = inferenced['tokens']\n",
    "    pos = inferenced['part_of_speech_tags']\n",
    "    \n",
    "    my_tok.append(' '.join([t for t, p in zip(chain(*tokens), chain(*pos)) if (p in pos_set) and (t != ' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hanlp.txt', 'w') as f:\n",
    "    for t in my_tok:\n",
    "        print(t, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['兰蔻 净澈 焕肤 双重 精华 水 极光水 惠选 套组 8 折 划算',\n",
       " '雅诗兰黛 红石榴 洁面 选套 组  洗完 油田 肤质',\n",
       " 'SK-II 晶透 经典礼 盒  回购 神仙水 混 油皮 感觉 干',\n",
       " '兰蔻新 清滢 柔肤水 粉水 ml 惠选 套组 买 买 瓶子 瓶盖 之类 差 粉水 香',\n",
       " '雅诗兰黛 鲜活 亮采红 石榴 倍润水 惠选 套组    买 很多 快递 盒子 加 原产品 包装 寄过来 包装盒 塑料纸 塑封 拿 打开 试用 一点点 退回去 行 礼盒 以为 地摊 买 一 堆 回来 ...',\n",
       " '资生堂 新艳阳 夏臻效水 动力 防晒乳 优惠 套组 七折半 优惠',\n",
       " '用 啥 抗 老 产品 混油 用 神仙水 大红瓶 喜欢 雅诗兰黛 原生液 感觉 皮肤 变滑 用 hr 绿宝瓶 种 草 橘  ... 江湖 骗子 说 用 橘灿 可以 绿宝瓶 2 小样 出来 小 棕瓶 维稳 细水 长流 感觉 高能',\n",
       " '雅诗兰黛 小棕瓶 熬 眼霜 修护 滋养 套装  用 7 瓶 滋润度 用 放弃 会 回购 款 眼霜',\n",
       " '兰蔻 肌底 修护 舒润 精华 奢美 礼盒 差评 拧开 滴灌 坏 蓝色 精华液 出不来',\n",
       " '雅诗兰黛 小 棕瓶 & 修护 精华 惠选 套组    发 一 个 赠品 评论 限制 评论',\n",
       " '资生堂 洗颜 专科 绵润 水感 卸妆油 ... 真假 知道 小孩 喝 行 希望',\n",
       " '兰蔻 塑颜 紧致 焕白 霜  ml 惠选 套组 A 个 套装 抢 刷 好几 天 刷到 货 买完 没 货 抢手 赞赞',\n",
       " '雅诗兰黛 小棕 瓶眼 精华 日夜 修护 套装    套装 划算',\n",
       " '玫珂菲 保湿 卸妆霜    时候 店里 一 个 小 哥哥 店员 推荐 用 兰蔻 卸妆乳 用 玫珂菲 之后 爱上 卸妆',\n",
       " '兰蔻 塑颜 紧致 焕白 霜  ml 惠选 套组 A    雪花霜 回购 五六 瓶 活动 黑金卡 特惠 专用 抢 没 抢到 货 到 付款 没 退而求其次 抢 个 套装 合算',\n",
       " '兰蔻 精华 肌底 眼霜 惠选 套组 个 膏体 弄 眼睛 里头 辣 眼睛 会 回购',\n",
       " '雅诗兰黛 鲜活 亮采红 石榴 二合一 洁面乳    次 买 没 用 个 牌子 眼霜 洗面奶 应该',\n",
       " '雅诗兰黛 特润 修护 精华 眼霜 惠选 套组 送 紧塑 精华素',\n",
       " '雅诗兰黛 特润 修护 浓缩 精华 素惠 选 套组 说 赠品 漏 敷衍 高能 小 棕瓶 赶上 打折 囤货 回来 赠品 精华 小 包装盒 变色 打开 看 瓶子 干 黄色 液体 拧开 看 瓶口 哎 知道 发 图片 没 写 评价 找',\n",
       " 'SK-II 舒透 护肤 洁面霜    没用 信任 丝芙兰 网购 要 出 问题 内牛 满面 感恩 几 年 买 过期 不少 报答 买 活动 随缘 长长久久',\n",
       " '兰蔻 新 清滢 柔肤水 粉水 ml 惠选 套组    爱 凉儿 推荐 兰蔻 粉水 赠品 给力 凑上 丝芙兰黑卡 性价比 囤 对不起 钱包',\n",
       " '雪花秀 玉璨 净柔 面膜 惠选 套组    面膜 囤货 没有 用 增 打折 面膜 磁条 邮给 背 出去 逛街 走到 响到',\n",
       " '雅诗兰黛 小 棕瓶 50 ml 惠选 套组 这次 丝芙兰 让 怀疑 买 真货 产品 盒子 塌 说明书 发抖 ....',\n",
       " '丝芙兰 焕彩 琉璃 防水 眼线液    用 兰蔻 天鹅颈 眼线液 导购 推荐 款 替代 兰蔻 性价比 输 大牌',\n",
       " '兰蔻 持妆 轻透 粉底液    色号  用 小样 入 油痘肌',\n",
       " '掌握 选品 方法论 罗永浩 苏宁 购 直播 补货 33 次    大 数据 显示 次 直播 罗永浩 苏宁 直播 女性 观众 比例 提升 高峰 时期 达到 42% 女性 观众 增加 一部分 男性 观众 购买 美妆 产品 导致 SK-II 神仙水 商品 上架 1 分钟 抢空  ...',\n",
       " '初代 萌主 玩 京东 美妆 超 品日 百变妆 备 成就 独特 魅力 库洛牌 星星 魔法 棒 守护 神 小 可 国民 少女 漫 魔卡 少女 樱 让 个 Z 世代 童年 做 一 个 魔法 梦 年岁 情怀 永存 爱 做梦 少年 少女 长大成人 妨碍 木之本樱 �� ...',\n",
       " '兰蔻 小 黑瓶 可以 代替 精华 兰蔻 小 黑瓶 适合 人群    大部分 人 喜欢 购买 兰蔻 小 黑瓶 口碑 款式 新颖 皮肤 伤害 购买 人群 增加 兰蔻 小 黑瓶 可以 代替 精华 回答  ...',\n",
       " 'sk2 大 红瓶 面霜 功效 sk2 大 红瓶 面膜 保存    sk2 大 红瓶 面霜 全国 销量 价格 便宜 种类 使用 效果 显著 深受 大众 喜爱 sk 2 大 红瓶 面霜 功效 回答 内容 告诉 ...',\n",
       " '美业 一 周 要闻 性别 文化 促进 全球 男士 化妆品 业务 增长    美业 观察 期 内容 新 朋友 请 点 标题 蓝字 搜索 微信号 mygc 360 关注 美业 一 周 要 闻 周 选出 三 篇 新闻 做 点评 1 性别 文化 推动 全球 男士 化妆品 业务 增长',\n",
       " '销售 破 3541万 成都 大悦城 逆市 引爆 疯抢 狂欢    成都 2020 年 9 月 14 日 美通社 一 年 一 度 “  大悦 疯抢节 一 次 刷爆 朋友圈 活动 成都 大悦城 收获 3541万 元 销售 业绩 11.1万 人次 客流量 总 车流 突',\n",
       " '生态 启示录 淘 品牌 组团 上市 意味 2 年 天猫 双 11 韩都 衣舍 裂帛 茵曼 淘 品牌 言 场 硬仗 要 打 —— 面向 资本 市场 登陆 创业板 IPO 挂牌 新 三 板',\n",
       " '知名 韩妆 品牌 MISSHA 谜尚 携手 丽人 丽妆 掀 韩妆 热潮 知名 韩妆 品牌 MISSHA 谜尚 国内 领先 线上 美妆 代理 服务商 丽人 丽妆 达成 战略 协议 谜尚 总经理 杜玉卓 女士 丽人 丽妆 董事长 黄韬 先生 上海 签署 合作 协议 强强 联手 开启 深度 合作',\n",
       " '一 张 脸 要 花 几百万 贵妇 美容 账单    原 标题 一 张 脸 要 花 几百万 贵妇 美容 账单 疯狂 女人 保养 账单 能 疯狂 范主 看 一 项 统计 护肤品 单价 控制 千 元 女人 一生 基础 护肤 要 花掉 91万 明星 �  ...']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['兰蔻 净澈 焕肤 双重 精华 水 极光水 惠选 套组   8 折 划算',\n",
       " '雅诗兰黛 红石榴 洁面 选套 组  洗完 油田 肤质',\n",
       " 'SK-II   晶透 经典礼 盒  回购 神仙水 混 油皮 感觉 干',\n",
       " '兰蔻新 清滢 柔肤水 粉水 ml 惠选 套组   买 买 瓶子 瓶盖 之类 差 粉水 香',\n",
       " '雅诗兰黛 鲜活 亮采红 石榴 倍润水 惠选 套组    买 很多 快递 盒子 加 原产品 包装 寄过来 包装盒 塑料纸 塑封 拿 打开 试用 一点点 退回去 行 礼盒 以为 地摊 买 一 堆 回来 ...',\n",
       " '资生堂 新艳阳 夏臻效水 动力 防晒乳 优惠 套组 七折半 优惠',\n",
       " '用 啥 抗 老 产品   混油 用 神仙水 大红瓶 喜欢 雅诗兰黛 原生液 感觉 皮肤 变滑 用 hr 绿宝瓶 种 草 橘  ... 江湖 骗子   说 用 橘灿 可以 绿宝瓶 2 小样 出来 小 棕瓶 维稳 细水 长流 感觉 高能',\n",
       " '雅诗兰黛 小棕瓶 熬 眼霜 修护 滋养 套装  用 7 瓶 滋润度 用 放弃 会 回购 款 眼霜',\n",
       " '兰蔻 肌底 修护 舒润 精华 奢美 礼盒 差评   拧开 滴灌 坏   蓝色 精华液 出不来',\n",
       " '雅诗兰黛 小 棕瓶 & 修护 精华 惠选 套组    发 一 个 赠品 评论 限制 评论',\n",
       " '资生堂   洗颜 专科 绵润 水感 卸妆油 ... 真假 知道 小孩 喝 行 希望',\n",
       " '兰蔻 塑颜 紧致 焕白 霜  ml 惠选 套组 A   个 套装 抢 刷 好几 天 刷到 货   买完 没 货   抢手 赞赞  ',\n",
       " '雅诗兰黛 小棕 瓶眼 精华 日夜 修护 套装    套装 划算',\n",
       " '玫珂菲 保湿 卸妆霜    时候 店里 一 个 小 哥哥 店员 推荐 用 兰蔻 卸妆乳 用 玫珂菲 之后 爱上 卸妆',\n",
       " '兰蔻 塑颜 紧致 焕白 霜  ml 惠选 套组 A    雪花霜 回购 五六 瓶 活动 黑金卡 特惠 专用 抢 没 抢到 货 到 付款 没 退而求其次 抢 个 套装 合算',\n",
       " '兰蔻 精华 肌底 眼霜 惠选 套组 个 膏体 弄 眼睛 里头 辣 眼睛 会 回购',\n",
       " '雅诗兰黛 鲜活 亮采红 石榴 二合一 洁面乳    次 买 没 用 个 牌子 眼霜 洗面奶 应该',\n",
       " '雅诗兰黛 特润 修护 精华 眼霜 惠选 套组 送 紧塑 精华素',\n",
       " '雅诗兰黛 特润 修护 浓缩 精华 素惠 选 套组 说 赠品 漏 敷衍 高能 小 棕瓶 赶上 打折 囤货 回来 赠品 精华 小 包装盒 变色 打开 看 瓶子 干 黄色 液体 拧开 看 瓶口 哎 知道 发 图片 没 写 评价 找',\n",
       " 'SK-II   舒透 护肤 洁面霜    没用 信任 丝芙兰 网购 要 出 问题 内牛 满面 感恩 几 年 买 过期 不少 报答 买 活动 随缘 长长久久',\n",
       " '兰蔻 新 清滢 柔肤水 粉水 ml 惠选 套组    爱 凉儿 推荐 兰蔻 粉水 赠品 给力 凑上 丝芙兰黑卡 性价比 囤 对不起 钱包',\n",
       " '雪花秀 玉璨 净柔 面膜 惠选 套组    面膜 囤货 没有 用 增 打折 面膜 磁条 邮给 背 出去 逛街 走到 响到',\n",
       " '雅诗兰黛 小 棕瓶 50 ml 惠选 套组 这次 丝芙兰 让 怀疑 买 真货 产品 盒子 塌 说明书 发抖 ....',\n",
       " '丝芙兰 焕彩 琉璃 防水 眼线液    用 兰蔻 天鹅颈 眼线液 导购 推荐 款 替代 兰蔻 性价比 输 大牌',\n",
       " '兰蔻 持妆 轻透 粉底液    色号  用 小样 入   油痘肌',\n",
       " '掌握 选品 方法论 罗永浩 苏宁 购 直播 补货 33 次    大 数据 显示 次 直播 罗永浩 苏宁 直播 女性 观众 比例 提升 高峰 时期 达到 42% 女性 观众 增加 一部分 男性 观众 购买 美妆 产品 导致 SK-II 神仙水 商品 上架 1 分钟 抢空  ...',\n",
       " '初代 萌主 玩 京东 美妆 超 品日 百变妆 备 成就 独特 魅力 库洛牌 星星 魔法 棒 守护 神 小 可 国民 少女 漫 魔卡 少女 樱 让 个 Z 世代 童年 做 一 个 魔法 梦 年岁 情怀 永存 爱 做梦 少年 少女 长大成人 妨碍 木之本樱 �� ...',\n",
       " '兰蔻 小 黑瓶 可以 代替 精华 兰蔻 小 黑瓶 适合 人群    大部分 人 喜欢 购买 兰蔻 小 黑瓶 口碑 款式 新颖 皮肤 伤害 购买 人群 增加 兰蔻 小 黑瓶 可以 代替 精华 回答  ...',\n",
       " 'sk2 大 红瓶 面霜 功效 sk2 大 红瓶 面膜 保存    sk2 大 红瓶 面霜 全国 销量 价格 便宜 种类 使用 效果 显著 深受 大众 喜爱 sk 2 大 红瓶 面霜 功效 回答 内容 告诉   ...',\n",
       " '美业 一 周 要闻 性别 文化 促进 全球 男士 化妆品 业务 增长    美业 观察   期 内容 新 朋友 请 点 标题 蓝字 搜索 微信号   mygc 360   关注 美业 一 周 要 闻 周 选出 三 篇 新闻 做 点评   1 性别 文化 推动 全球 男士 化妆品 业务 增长',\n",
       " '销售 破 3541万 成都 大悦城 逆市 引爆 疯抢 狂欢    成都 2020 年 9 月 14 日   美通社     一 年 一 度   “  大悦 疯抢节   一 次 刷爆 朋友圈 活动 成都 大悦城 收获   3541万 元   销售 业绩   11.1万 人次   客流量 总 车流 突',\n",
       " '生态 启示录 淘 品牌 组团 上市 意味 2   年   天猫 双 11 韩都 衣舍 裂帛 茵曼 淘 品牌 言 场 硬仗 要 打 —— 面向 资本 市场 登陆 创业板 IPO 挂牌 新 三 板',\n",
       " '知名 韩妆 品牌 MISSHA 谜尚 携手 丽人 丽妆 掀 韩妆 热潮   知名 韩妆 品牌 MISSHA 谜尚 国内 领先 线上 美妆 代理 服务商 丽人 丽妆 达成 战略 协议 谜尚 总经理 杜玉卓 女士 丽人 丽妆 董事长 黄韬 先生 上海 签署 合作 协议 强强 联手 开启 深度 合作',\n",
       " '一 张 脸 要 花 几百万 贵妇 美容 账单    原 标题 一 张 脸 要 花 几百万 贵妇 美容 账单 疯狂 女人 保养 账单 能 疯狂 范主 看 一 项 统计 护肤品 单价 控制 千 元 女人 一生 基础 护肤 要 花掉 91万 明星 �  ...']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[''.join(x) for x in my_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['兰蔻 净澈 焕肤 双重 精华 水 极光水 惠选 套组   8 折 划算',\n",
       " '雅诗兰黛 红石榴 洁面 选套 组  洗完 油田 肤质',\n",
       " 'SK-II   晶透 经典礼 盒  回购 神仙水 混 油皮 适合 感觉 干',\n",
       " '兰蔻新 清滢 柔肤水 粉水 ml 惠选 套组   买 , 买 好 无论 瓶子 , 瓶盖 之类 差 , 粉水 浓 , 香',\n",
       " '雅诗兰黛 鲜活 亮采红 石榴 倍润水 惠选 套组    买 很多 快递 盒子 加 原产品 包装 寄过来 包装盒 塑料纸 塑封 拿 打开 试用 一点点 退回去 行 礼盒 感觉 以为 地摊 买 一 堆 回来 ...',\n",
       " '资生堂 新艳阳 夏臻效水 动力 防晒乳 优惠 套组 七折半 优惠',\n",
       " '用 啥 抗 老 产品   混油 用 神仙水 + 大红瓶 喜欢 雅诗兰黛 原生液 感觉 皮肤 变滑 用 hr 绿宝瓶 种 草 橘  ... 江湖 骗子   说 用 , 橘灿 可以 , 绿宝瓶 2 小样 出来 小 棕瓶 维稳 细水 长流 感觉 高能 好一点',\n",
       " '雅诗兰黛 小棕瓶 熬 眼霜 修护 滋养 套装  用 7 瓶 , 滋润度 够 用 干 , 放弃 会 回购 款 眼霜',\n",
       " '兰蔻 肌底 修护 舒润 精华 奢美 礼盒 差评   拧开 滴灌 坏   蓝色 精华液 出不来',\n",
       " '雅诗兰黛 小 棕瓶   修护 精华 惠选 套组    发 一 个 赠品 评论 限制 评论',\n",
       " '资生堂   洗颜 专科 绵润 水感 卸妆油 ... 真假 知道 小孩 喝 行 希望',\n",
       " '兰蔻 塑颜 紧致 焕白 霜  ml 惠选 套组 A   个 套装 抢 刷 好几 天 , 刷到 货 买完 没 货   抢手 赞赞  ',\n",
       " '雅诗兰黛 小棕 瓶眼 精华 日夜 修护 套装    套装 划算',\n",
       " '玫珂菲 保湿 卸妆霜    好用 时候 店里 一 个 小 哥哥 店员 推荐 用 兰蔻 卸妆乳 用 玫珂菲 之后 爱上 卸妆',\n",
       " '兰蔻 塑颜 紧致 焕白 霜  ml 惠选 套组 A    雪花霜 回购 五六 瓶 活动 黑金卡 特惠 专用 抢 没 抢到 货 , 到 付款 没 退而求其次 抢 个 套装 合算',\n",
       " '兰蔻 精华 肌底 眼霜 惠选 套组 个 膏体 弄 眼睛 里头 辣 眼睛 会 回购',\n",
       " '雅诗兰黛 鲜活 亮采红 石榴 二合一 洁面乳    次 买 , 没 用 , 个 牌子 眼霜 洗面奶 应该',\n",
       " '雅诗兰黛 特润 修护 精华 眼霜 惠选 套组 送 紧塑 精华素',\n",
       " '雅诗兰黛 特润 修护 浓缩 精华 素惠 选 套组 说 赠品 漏 敷衍 高能 小 棕瓶 , 赶上 打折 囤货 结果 回来 赠品 精华 小 包装盒 变色 , 打开 一 看 瓶子 干 黄色 液体 , 拧开 一 看 瓶口 , 哎 知道 发 图片 没 写 评价 找',\n",
       " 'SK-II   舒透 护肤 洁面霜    没用 信任 丝芙兰 网购 要 出 问题 内牛 满面 感恩 几 年 买 过期 , 报答 买 活动 随缘 长长久久',\n",
       " '兰蔻 新 清滢 柔肤水 粉水 ml 惠选 套组    爱 凉儿 推荐 兰蔻 粉水 赠品 给力 凑上 丝芙兰黑卡 性价比 高 囤 对不起 钱包',\n",
       " '雪花秀 玉璨 净柔 面膜 惠选 套组    面膜 囤货 用 , 增 打折 就是 面膜 磁条 邮给 背 出去 逛街 走到 响到 ,',\n",
       " '雅诗兰黛 小 棕瓶 50 ml 惠选 套组    这次 丝芙兰 让 怀疑 买 真货 产品 盒子 塌 , 说明书 瑟瑟 发抖 ....',\n",
       " '丝芙兰 焕彩 琉璃 防水 眼线液    用 兰蔻 天鹅颈 眼线液 导购 推荐 款 替代 兰蔻 性价比 输 大牌',\n",
       " '兰蔻 持妆 轻透 粉底液    色号  用 小样 入   油痘肌',\n",
       " '掌握 选品 方法论 罗永浩 苏宁 购 直播 补货 33 次    大 数据 显示 次 直播 罗永浩 苏宁 直播 女性 观众 比例 提升 高峰 时期 达到 42% 女性 观众 增加 , 一部分 男性 观众 购买 美妆 产品 导致 SK-II 神仙水 商品 上架 1 分钟 抢空  ...',\n",
       " '初代 萌主 玩 京东 美妆 超 品日 百变妆 备 成就 独特 魅力 库洛牌 星星 魔法 棒 守护 神 小 可...... 国民 少女 漫 魔卡 少女 樱 让 个 Z 世代 童年 做 一 个 魔法 梦 年岁 , 情怀 永存 爱 做梦 少年 少女 长大成人 妨碍 木之本樱   ...',\n",
       " '兰蔻 小 黑瓶 可以 代替 精华 兰蔻 小 黑瓶 适合 人群    大部分 人 喜欢 购买 兰蔻 小 黑瓶 口碑 款式 新颖 皮肤 伤害 购买 人群 增加 兰蔻 小 黑瓶 可以 代替 精华 回答  ...',\n",
       " 'sk2 大 红瓶 面霜 功效 sk2 大 红瓶 面膜 保存    sk2 大 红瓶 面霜 全国 销量 , 价格 便宜 , 种类 多 , 使用 效果 显著 深受 大众 喜爱 sk 2 大 红瓶 面霜 功效 回答 内容 告诉   ...',\n",
       " '美业 一 周 要 闻 性别 文化 促进 全球 男士 化妆品 业务 增长   美业 观察   期 内容   新 朋友 请 点 标题 蓝字 搜索 信号   mygc 360   关注 美业 一 周 要 闻 周 选出 三 篇 新闻 做 点评    1 性别 文化 推动 全球 男士 化妆品 业务 增长',\n",
       " '销售 破 3541万 成都 大悦城 逆市 引爆 疯抢 狂欢    成都 2020 年 9 月 14 日   美通社     一 年 一 度   “  大悦 疯抢节   一 次 刷爆 朋友圈 活动 成都 大悦城 收获   3541万 元 销售 业绩   11.1万 人次   客流量 总 车流 突',\n",
       " '生态 启示录  淘 品牌 组团 上市 意味 2   年   天猫 双 11 韩都 衣舍 裂帛 茵曼 淘 品牌 言 场 硬仗 要 打   面向 资本 市场 登陆 创业板 IPO 挂牌 新 三 板',\n",
       " '知名 韩妆 品牌 MISSHA 谜尚 携手 丽人 丽妆 掀 韩妆 热潮 知名 韩妆 品牌 MISSHA 谜尚 国内 领先 线上 美妆 代理 服务商 丽人 丽妆 达成 战略 协议 谜尚 总经理 杜玉卓 女士 丽人 丽妆 董事长 黄韬 先生 上海 签署 合作 协议 联手 开启 深度 合作',\n",
       " '一 张 脸 要 花 几百万 贵妇 美容 账单 原 标题   一 张 脸 要 花 几百万 贵妇 美容 账单 疯狂 女人 保养 账单 能 疯狂 范主 看 一 项 统计 , 护肤品 单价 控制 千 元 , 女人 一生 基础 护肤 要 花掉 91万 明星    ...']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'兰蔻 净澈 焕肤 双重 精华 水 极光水 惠选 套组   8 折 划算'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "兰蔻 NN\n",
      "净澈 NN\n",
      "焕肤 NN\n",
      "双重 JJ\n",
      "精华 NN\n",
      "水 NN\n",
      "( PU\n",
      "极光水 NN\n",
      ") PU\n",
      "惠选 NR\n",
      "套组 NN\n",
      ", PU\n",
      "  CD\n",
      "8 NN\n",
      "折 VV\n",
      "挺 AD\n",
      "划算 NN\n"
     ]
    }
   ],
   "source": [
    "for tok, p in zip(chain(*tokens), chain(*pos)):\n",
    "    print(tok, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['兰蔻', '净澈', '焕肤', '双重', '精华', '水', '(', '极光水', ')', '惠选', '套组', ',', ' ', '8', '折', '挺', '划算'], ['NN', 'NN', 'NN', 'JJ', 'NN', 'NN', 'PU', 'NN', 'PU', 'NR', 'NN', 'PU', 'CD', 'NN', 'VV', 'AD', 'NN'])\n"
     ]
    }
   ],
   "source": [
    "for x in zip(tokens, pos):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NN',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  'PU',\n",
       "  'NN',\n",
       "  'PU',\n",
       "  'NR',\n",
       "  'NN',\n",
       "  'PU',\n",
       "  'CD',\n",
       "  'NN',\n",
       "  'VV',\n",
       "  'AD',\n",
       "  'NN']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['兰蔻',\n",
       "  '净澈',\n",
       "  '焕肤',\n",
       "  '双重',\n",
       "  '精华',\n",
       "  '水',\n",
       "  '(',\n",
       "  '极光水',\n",
       "  ')',\n",
       "  '惠选',\n",
       "  '套组',\n",
       "  ',',\n",
       "  ' ',\n",
       "  '8',\n",
       "  '折',\n",
       "  '挺',\n",
       "  '划算']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ys.txt') as f:\n",
    "    ys = f.read().rstrip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lac.txt') as f:\n",
    "    lac = f.read().rstrip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hanlp.txt') as f:\n",
    "    hanlp = f.read().rstrip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "0.2857142857142857\n",
      "0.7777777777777778\n",
      "0.46153846153846156\n",
      "0.375\n",
      "0.6\n",
      "0.5714285714285714\n",
      "0.7692307692307693\n",
      "0.4\n",
      "0.5\n",
      "0.6363636363636364\n",
      "0.4666666666666667\n",
      "0.2\n",
      "0.6923076923076923\n",
      "0.45454545454545453\n",
      "0.38461538461538464\n",
      "0.6428571428571429\n",
      "0.14285714285714285\n",
      "0.6451612903225806\n",
      "0.5652173913043478\n",
      "0.5625\n",
      "0.6\n",
      "0.5714285714285714\n",
      "0.42857142857142855\n",
      "0.5714285714285714\n",
      "0.6666666666666666\n",
      "0.4444444444444444\n",
      "0.6111111111111112\n",
      "0.6521739130434783\n",
      "0.7083333333333334\n",
      "0.84\n",
      "0.5517241379310345\n",
      "0.84\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# ys and lac\n",
    "for y, l in zip(ys, lac):\n",
    "    ys_set = set(y.split())\n",
    "    lac_set = set(l.split())\n",
    "    print(len(ys_set & lac_set) / len(ys_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ys and lac\n",
    "container = []\n",
    "for y, l in zip(ys, lac):\n",
    "    ys_set = set(y.split())\n",
    "    lac_set = set(l.split())\n",
    "    container.append(len(ys_set & lac_set) / len(ys_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.556215603298683"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "55.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ys and hanlp\n",
    "container = []\n",
    "for y, l in zip(ys, hanlp):\n",
    "    ys_set = set(y.split())\n",
    "    hanlp_set = set(l.split())\n",
    "    container.append(len(ys_set & hanlp_set) / len(ys_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5473067050568516"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${1 \\over L} \\sum_i^L {N(y_i \\cap \\widehat{y}_i) \\over N(y_i)}$$\n",
    "- L은 평가하는 문장의 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['兰蔻 净澈焕肤 双重 精华水 极光水 惠选套组 8折 划算\\n雅诗兰黛 红石榴 洁面 惠选套组 清爽 油田肤质 友好\\nSK-II 晶透 经典礼盒 回购 神仙水 混油皮 适合 冬天 干\\n兰蔻 新清 滢柔肤水 粉水 400ml 惠选套组 没上次 好 瓶子 瓶盖 差 粉水 没上次 浓 香\\n雅诗兰黛 鲜活亮采红石榴倍润水 惠选套组 买 快递盒子 原产品包装 寄 包装盒 没 塑封 试用一点 再退 没礼盒 不好 以为 地摊上 买\\n资生堂 新艳阳夏臻效水动力防晒乳 优惠套组 七折半 优惠\\n混油 用 神仙水 大红瓶 冬天 雅诗兰黛 原生液 变滑 hr绿宝瓶 种草橘 橘灿 还可以 绿宝瓶 2只 小样 感觉不出来 小棕瓶 维稳 太细水长流了 高能 好\\n雅诗兰黛 小棕瓶 熬眼霜修护滋养 套装 用 7瓶 滋润度 不够 夏天 用 很干 放弃 不 回购\\n兰蔻 肌底 修护舒润 精华 奢美礼盒 差评 滴灌 坏的 蓝色精华液 出不来\\n雅诗兰黛 小棕瓶 修护精华 惠选套组 少 赠品，不评论了 限制 评论\\n资生堂 洗颜专科 绵润水感 卸妆油. 真假 不知道 小孩 喝 还行 希望 是真的\\n兰蔻 塑颜紧致 焕白霜 50ml 惠选套组A 套装 得抢 刷 好几天 有货 买完 没货 抢手 赞赞 合适\\n雅诗兰黛 小棕瓶 眼精华 日夜修护套装 划算\\n玫珂菲 保湿 卸妆霜 好用 店员 推荐 之前 用 兰蔻 卸妆乳 玫珂菲 爱上 卸妆 干净\\n兰蔻 塑颜 紧致 焕白霜 50ml 惠选套组A 雪花霜 回购 五 六瓶 活动 黑金卡 特惠 专用 难 抢 付款 没 退求 抢了 套装 合算\\n兰蔻 精华 肌底 眼霜 惠选套组  膏体 弄 眼睛 里 辣 眼睛 不 再 回购\\n雅诗兰黛 鲜活亮采红石榴 二合一 洁面乳 第一次 买 没用 一直 用 这 眼霜 洗面奶 应该 不错\\n雅诗兰黛 特润 修护精华眼霜 惠选套组 送的 紧塑精华素 小\\n雅诗兰黛 特润密集修护 浓缩精华素 惠选套组 赠品 漏 敷衍了 高能 小棕瓶 好用 打折 囤货 赠品 精华小包装盒 变色了 打开 看 瓶子 干了的 黄色 液体 拧开 看 瓶口 不知道 发 图片之前 没 写 评价 找 半天 找到\\nSK-II 舒透 护肤 洁面霜 没用 信任 除 丝芙兰 网购 出 问题 内牛满面 感恩 早几年 买 多 过期不少 报答 缺啥 买啥 活动 随缘 长长久久\\n兰蔻 新清滢柔肤水 粉水 400ml 惠选套组 爱了 推荐的 兰蔻粉水 赠品 给力 丝芙兰 黑卡 性价比 高 不囤对不起 钱包\\n雪花秀 玉璨净柔面膜 惠选套组 面膜 囤货 没用 打折 划算 面膜 磁条邮给 晚上 出去 走 哪 响 哪 尴尬\\n雅诗兰黛 小棕瓶 50ml 惠选套组 丝芙兰 怀疑 真货 产品 盒子 塌的 说明书 没有 瑟瑟 发抖\\n丝芙兰 焕彩琉璃 防水眼线液 一直用 兰蔻 天鹅颈 眼线液 导购 推荐这款 替代 兰蔻 性价比 高 不输 大牌\\n兰蔻持妆 轻透粉底液 色号 满意的 小样 轻薄 油痘肌\\n选品 方法论 罗永浩 苏宁 易购 直播 补货 33次 数据 显示 直播 罗永浩 苏宁 直播间 女性观众 比例 提升 高峰 42% 女性观众 增加 男性观众 购买 美妆产品 SK-II 神仙水 1分钟 抢空\\n萌主 嗨玩 京东 美妆 超品日 百变妆备 独特魅力 库洛牌 星星魔法棒 守护神 小可 国民 少女漫 魔卡少女樱 Z世代 童年 做过 魔法梦 年岁渐长 情怀 永存 爱 做梦 少年少女 长大成人 不妨碍 木之本樱\\n兰蔻 小黑瓶 代替 精华 吗 兰蔻 小黑瓶 适合 人群 大部分 购买 兰蔻 小黑瓶 口碑 好 款式 新颖 对皮肤没 伤害 购买的 增加 兰蔻 小黑瓶 代替 精华吗\\nsk2 大红瓶 面霜 功效 sk2 大红瓶面膜 怎么保存 sk2 大红瓶 面霜 在全国 销量 可观 价格 便宜 种类 多使用 效果 显著 深受 大众 喜爱 sk2 大红瓶 面霜 功效 有 哪些 以下 回答 内容\\n美业 一周 要闻 无性别文化 促进男士 化妆品 业务 增长 美业 观察内容 点标 蓝字 搜索 微信号 mygc360 关注 美业 一周 要闻 本周 三篇 新闻 点评 1 无性别 文化 推动 男士 化妆品 业务 增长\\n日 销售 破 3541万 成都 大悦城 逆市 引爆 疯抢 狂欢 成都 2020年9月14日 美通社 周末 大悦疯抢节 刷爆 朋友圈 当天 成都 大悦城 收获 3541万元 销售 业绩 11.1万 客流量 总 车流 突破\\n生态 启示录 淘品 牌组团 上市 意味 什么 2 年前 除 天猫双11 对于 韩都衣舍 裂帛 茵曼 淘 品牌 今年 硬仗 要 打 面向 资本 市场 登陆 创业板 IPO 挂牌 新三板\\n知名 韩妆 品牌 MISSHA 谜尚 即将 携手 丽人丽妆 再掀 韩妆 热潮 近日 知名 韩妆 品牌 MISSHA 谜尚 国内 线上 美妆 代理 服务商 丽人丽妆 达成 协议 谜尚 总经理 杜玉卓 丽人丽妆 董事长 黄韬 上海签署 合作 协议 联手 合作\\n脸 花 几百万 贵妇 美容 账单 疯狂  原标题 脸 花 几百万 贵妇 美容 账单 疯狂 女人 “保养账单” 多疯狂 范主 看过 统计 护肤品 单价 控制千元 内 女人 一生 基础 护肤 要花 91万 明星']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'兰蔻 净澈 焕肤 双重 精华 水 极光水 惠选 套组 8 折 划算\\n雅诗兰黛 红石榴 洁面 选套 组  洗完 油田 肤质\\nSK-II 晶透 经典礼 盒  回购 神仙水 混 油皮 感觉 干\\n兰蔻新 清滢 柔肤水 粉水 ml 惠选 套组 买 买 瓶子 瓶盖 之类 差 粉水 香\\n雅诗兰黛 鲜活 亮采红 石榴 倍润水 惠选 套组    买 很多 快递 盒子 加 原产品 包装 寄过来 包装盒 塑料纸 塑封 拿 打开 试用 一点点 退回去 行 礼盒 以为 地摊 买 一 堆 回来\\n资生堂 新艳阳 夏臻效水 动力 防晒乳 优惠 套组 七折半 优惠\\n用 啥 抗 老 产品 混油 用 神仙水 大红瓶 喜欢 雅诗兰黛 原生液 感觉 皮肤 变滑 用 hr 绿宝瓶 种 草 橘 江湖 骗子 说 用 橘灿 可以 绿宝瓶 2 小样 出来 小 棕瓶 维稳 细水 长流 感觉 高能\\n雅诗兰黛 小棕瓶 熬 眼霜 修护 滋养 套装  用 7 瓶 滋润度 用 放弃 会 回购 款 眼霜\\n兰蔻 肌底 修护 舒润 精华 奢美 礼盒 差评 拧开 滴灌 坏 蓝色 精华液 出不来\\n雅诗兰黛 小 棕瓶 & 修护 精华 惠选 套组    发 一 个 赠品 评论 限制 评论\\n资生堂 洗颜 专科 绵润 水感 卸妆油 真假 知道 小孩 喝 行 希望\\n兰蔻 塑颜 紧致 焕白 霜  ml 惠选 套组 A 个 套装 抢 刷 好几 天 刷到 货 买完 没 货 抢手 赞赞\\n雅诗兰黛 小棕 瓶眼 精华 日夜 修护 套装    套装 划算\\n玫珂菲 保湿 卸妆霜    时候 店里 一 个 小 哥哥 店员 推荐 用 兰蔻 卸妆乳 用 玫珂菲 之后 爱上 卸妆\\n兰蔻 塑颜 紧致 焕白 霜  ml 惠选 套组 A    雪花霜 回购 五六 瓶 活动 黑金卡 特惠 专用 抢 没 抢到 货 到 付款 没 退而求其次 抢 个 套装 合算\\n兰蔻 精华 肌底 眼霜 惠选 套组 个 膏体 弄 眼睛 里头 辣 眼睛 会 回购\\n雅诗兰黛 鲜活 亮采红 石榴 二合一 洁面乳    次 买 没 用 个 牌子 眼霜 洗面奶 应该\\n雅诗兰黛 特润 修护 精华 眼霜 惠选 套组 送 紧塑 精华素\\n雅诗兰黛 特润 修护 浓缩 精华 素惠 选 套组 说 赠品 漏 敷衍 高能 小 棕瓶 赶上 打折 囤货 回来 赠品 精华 小 包装盒 变色 打开 看 瓶子 干 黄色 液体 拧开 看 瓶口 哎 知道 发 图片 没 写 评价 找\\nSK-II 舒透 护肤 洁面霜    没用 信任 丝芙兰 网购 要 出 问题 内牛 满面 感恩 几 年 买 过期 不少 报答 买 活动 随缘 长长久久\\n兰蔻 新 清滢 柔肤水 粉水 ml 惠选 套组    爱 凉儿 推荐 兰蔻 粉水 赠品 给力 凑上 丝芙兰黑卡 性价比 囤 对不起 钱包\\n雪花秀 玉璨 净柔 面膜 惠选 套组    面膜 囤货 没有 用 增 打折 面膜 磁条 邮给 背 出去 逛街 走到 响到\\n雅诗兰黛 小 棕瓶 50 ml 惠选 套组 这次 丝芙兰 让 怀疑 买 真货 产品 盒子 塌 说明书 发抖\\n丝芙兰 焕彩 琉璃 防水 眼线液    用 兰蔻 天鹅颈 眼线液 导购 推荐 款 替代 兰蔻 性价比 输 大牌\\n兰蔻 持妆 轻透 粉底液    色号  用 小样 入 油痘肌\\n掌握 选品 方法论 罗永浩 苏宁 购 直播 补货 33 次    大 数据 显示 次 直播 罗永浩 苏宁 直播 女性 观众 比例 提升 高峰 时期 达到 42% 女性 观众 增加 一部分 男性 观众 购买 美妆 产品 导致 SK-II 神仙水 商品 上架 1 分钟 抢空\\n初代 萌主 玩 京东 美妆 超 品日 百变妆 备 成就 独特 魅力 库洛牌 星星 魔法 棒 守护 神 小 可 国民 少女 漫 魔卡 少女 樱 让 个 Z 世代 童年 做 一 个 魔法 梦 年岁 情怀 永存 爱 做梦 少年 少女 长大成人 妨碍 木之本樱 \\n兰蔻 小 黑瓶 可以 代替 精华 兰蔻 小 黑瓶 适合 人群    大部分 人 喜欢 购买 兰蔻 小 黑瓶 口碑 款式 新颖 皮肤 伤害 购买 人群 增加 兰蔻 小 黑瓶 可以 代替 精华 回答\\nsk2 大 红瓶 面霜 功效 sk2 大 红瓶 面膜 保存    sk2 大 红瓶 面霜 全国 销量 价格 便宜 种类 使用 效果 显著 深受 大众 喜爱 sk 2 大 红瓶 面霜 功效 回答 内容 告诉\\n美业 一 周 要闻 性别 文化 促进 全球 男士 化妆品 业务 增长    美业 观察 期 内容 新 朋友 请 点 标题 蓝字 搜索 微信号 mygc 360 关注 美业 一 周 要 闻 周 选出 三 篇 新闻 做 点评 1 性别 文化 推动 全球 男士 化妆品 业务 增长\\n销售 破 3541万 成都 大悦城 逆市 引爆 疯抢 狂欢    成都 2020 年 9 月 14 日 美通社 一 年 一 度 “  大悦 疯抢节 一 次 刷爆 朋友圈 活动 成都 大悦城 收获 3541万 元 销售 业绩 11.1万 人次 客流量 总 车流 突\\n生态 启示录 淘 品牌 组团 上市 意味 2 年 天猫 双 11 韩都 衣舍 裂帛 茵曼 淘 品牌 言 场 硬仗 要 打 —— 面向 资本 市场 登陆 创业板 IPO 挂牌 新 三 板\\n知名 韩妆 品牌 MISSHA 谜尚 携手 丽人 丽妆 掀 韩妆 热潮 知名 韩妆 品牌 MISSHA 谜尚 国内 领先 线上 美妆 代理 服务商 丽人 丽妆 达成 战略 协议 谜尚 总经理 杜玉卓 女士 丽人 丽妆 董事长 黄韬 先生 上海 签署 合作 协议 强强 联手 开启 深度 合作\\n一 张 脸 要 花 几百万 贵妇 美容 账单    原 标题 一 张 脸 要 花 几百万 贵妇 美容 账单 疯狂 女人 保养 账单 能 疯狂 范主 看 一 项 统计 护肤品 单价 控制 千 元 女人 一生 基础 护肤 要 花掉 91万 明星 \\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SK-II 晶透经典礼盒 无限回购的神仙水，混油皮很适合，冬天用感觉稍有点干。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = {'CD', 'FW', 'IJ', 'JJ', 'M', 'NN', 'NR', 'VP', 'VV', 'X'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'NT', 'NR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEC'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(normalize(docs[2]))['part_of_speech_tags'][0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'的'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(normalize(docs[2]))['tokens'][0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': ['SK-II 晶透经典礼盒 无限回购的神仙水,混油皮很适合,冬天用感觉稍有点干。'],\n",
       " 'tokens': [['SK-II',\n",
       "   ' ',\n",
       "   '晶透',\n",
       "   '经典礼',\n",
       "   '盒 ',\n",
       "   '无限',\n",
       "   '回购',\n",
       "   '的',\n",
       "   '神仙水',\n",
       "   ',',\n",
       "   '混',\n",
       "   '油皮',\n",
       "   '很',\n",
       "   '适合',\n",
       "   ',',\n",
       "   '冬天',\n",
       "   '用',\n",
       "   '感觉',\n",
       "   '稍',\n",
       "   '有点',\n",
       "   '干',\n",
       "   '。']],\n",
       " 'part_of_speech_tags': [['NR',\n",
       "   'NR',\n",
       "   'NR',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'DEC',\n",
       "   'NN',\n",
       "   'PU',\n",
       "   'M',\n",
       "   'NN',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'PU',\n",
       "   'NT',\n",
       "   'P',\n",
       "   'NN',\n",
       "   'AD',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'PU']],\n",
       " 'ner_tags': [[('NR NR NR NN', 'NT', 0, 4),\n",
       "   ('NR NR NR NN NN', 'NT', 0, 5),\n",
       "   ('DEC', 'NT', 7, 8)]],\n",
       " '_ipython_canary_method_should_not_exist_': []}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(normalize(docs[2])) #['tokens'][0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': ['兰蔻净澈焕肤双重精华水(极光水)惠选套组, 8折挺划算'],\n",
       " 'tokens': [['兰蔻',\n",
       "   '净澈',\n",
       "   '焕肤',\n",
       "   '双重',\n",
       "   '精华',\n",
       "   '水',\n",
       "   '(',\n",
       "   '极光水',\n",
       "   ')',\n",
       "   '惠选',\n",
       "   '套组',\n",
       "   ',',\n",
       "   ' ',\n",
       "   '8',\n",
       "   '折',\n",
       "   '挺',\n",
       "   '划算']],\n",
       " 'part_of_speech_tags': [['NN',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   'PU',\n",
       "   'NN',\n",
       "   'PU',\n",
       "   'NR',\n",
       "   'NN',\n",
       "   'PU',\n",
       "   'CD',\n",
       "   'NN',\n",
       "   'VV',\n",
       "   'AD',\n",
       "   'NN']],\n",
       " 'ner_tags': [[]],\n",
       " '_ipython_canary_method_should_not_exist_': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    for i in range(len(tokens)):\n",
    "        for token, pos in zip(tokens[i], )\n",
    "\n",
    "pipeline(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_eagerly': True,\n",
       " 'epochs': 3,\n",
       " 'batch_size': 32,\n",
       " 'metrics': 'f1',\n",
       " 'max_seq_length': 128,\n",
       " 'use_amp': False,\n",
       " 'warmup_steps_ratio': 0,\n",
       " 'clipnorm': 1.0,\n",
       " 'epsilon': 1e-08,\n",
       " 'weight_decay_rate': 0,\n",
       " 'learning_rate': 5e-05,\n",
       " 'optimizer': {'class_name': 'AdamWeightDecay',\n",
       "  'config': {'name': 'AdamWeightDecay',\n",
       "   'clipnorm': 1.0,\n",
       "   'learning_rate': {'class_name': 'PolynomialDecay',\n",
       "    'config': {'initial_learning_rate': 5e-05,\n",
       "     'decay_steps': 3990,\n",
       "     'end_learning_rate': 0.0,\n",
       "     'power': 1.0,\n",
       "     'cycle': False,\n",
       "     'name': None}},\n",
       "   'decay': 0.0,\n",
       "   'beta_1': 0.9,\n",
       "   'beta_2': 0.999,\n",
       "   'epsilon': 1e-08,\n",
       "   'amsgrad': False,\n",
       "   'weight_decay_rate': 0}},\n",
       " 'transformer': 'albert_base_zh',\n",
       " 'warmup_steps': 0,\n",
       " 'train_steps': 3990,\n",
       " 'training': False,\n",
       " 'logger': <Logger hanlp (INFO)>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雅诗兰黛 NN\n",
      "鲜活 VV\n",
      "亮采红 NN\n",
      "石榴 NN\n",
      "倍润水 NN\n",
      "惠选 NN\n",
      "套组 NN\n",
      "   NN\n",
      "买 VV\n",
      "了 AS\n",
      "很多 CD\n",
      "， PU\n",
      "就 AD\n",
      "快递 NN\n",
      "盒子 NN\n",
      "加 VV\n",
      "原产品 NN\n",
      "包装 NN\n",
      "寄过来 VV\n",
      "。 PU\n",
      "包装盒 NN\n",
      "都 AD\n",
      "没有 VE\n",
      "塑料纸 NN\n",
      "塑封 NN\n",
      "， PU\n",
      "拿 VV\n",
      "着 AS\n",
      "直接 AD\n",
      "打开 VV\n",
      "了 AS\n",
      "试用 VV\n",
      "一点点 NN\n",
      "， PU\n",
      "再 AD\n",
      "退回去 VV\n",
      "就 AD\n",
      "行 VV\n",
      "。 PU\n",
      "也 AD\n",
      "没有 VE\n",
      "礼盒 NN\n",
      "， PU\n",
      "感觉 AD\n",
      "真 AD\n",
      "不 AD\n",
      "好 VA\n",
      "， PU\n",
      "还 AD\n",
      "以为 VV\n",
      "我 PN\n",
      "从 P\n",
      "地摊 NN\n",
      "上 LC\n",
      "买 VV\n",
      "了 AS\n",
      "一 CD\n",
      "堆 M\n",
      "回来 VV\n",
      "... NN\n"
     ]
    }
   ],
   "source": [
    "#for doc in docs:\n",
    "inferenced_doc = pipeline(docs[4])\n",
    "sentences = inferenced_doc['sentences']\n",
    "tokenized_doc = inferenced_doc['tokens']\n",
    "pos_doc = inferenced_doc['part_of_speech_tags']\n",
    "ner_doc = inferenced_doc['ner_tags']\n",
    "\n",
    "for token_sent, pos_sent, ner in zip(tokenized_doc, pos_doc, ner_doc):\n",
    "    for token, p in zip(token_sent, pos_sent):\n",
    "        print(token, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CD', 'FW', 'IJ', 'JJ', 'M', 'NN', 'NR', 'VP', 'VV', 'X'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp =['NN',\n",
    "   'VV',\n",
    "   'NN',\n",
    "   'NN',\n",
    "   'NN',\n",
    "   'NN',\n",
    "   'NN',\n",
    "   'NN',\n",
    "   'VV',\n",
    "   'AS',\n",
    "   'CD',\n",
    "   'PU',\n",
    "   'AD',\n",
    "   'NN',\n",
    "   'NN',\n",
    "   'VV',\n",
    "   'NN',\n",
    "   'NN',\n",
    "   'VV',\n",
    "   'PU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[3:8] = ['NT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN',\n",
       " 'VV',\n",
       " 'NN',\n",
       " 'NT',\n",
       " 'VV',\n",
       " 'AS',\n",
       " 'CD',\n",
       " 'PU',\n",
       " 'AD',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'VV',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'VV',\n",
       " 'PU']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name nr, place ns, orgnization nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': ['大家都用啥抗老产品啊 混油现在在用神仙水＋大红瓶，冬天喜欢雅诗兰黛原生液。',\n",
       "  '感觉皮肤真的变滑。',\n",
       "  '也用hr绿宝瓶。',\n",
       "  '种草橘 ...',\n",
       "  '江湖骗子 你说的我有用过，橘灿还可以，绿宝瓶2只小样感觉不出来啥，小棕瓶维稳太细水长流了，感觉高能好一点'],\n",
       " 'tokens': [['大家',\n",
       "   '都',\n",
       "   '用',\n",
       "   '啥',\n",
       "   '抗',\n",
       "   '老',\n",
       "   '产品',\n",
       "   '啊',\n",
       "   ' ',\n",
       "   '混油',\n",
       "   '现在',\n",
       "   '在',\n",
       "   '用',\n",
       "   '神仙水',\n",
       "   '＋',\n",
       "   '大红瓶',\n",
       "   '，',\n",
       "   '冬天',\n",
       "   '喜欢',\n",
       "   '雅诗兰黛',\n",
       "   '原生液',\n",
       "   '。'],\n",
       "  ['感觉', '皮肤', '真的', '变滑', '。'],\n",
       "  ['也', '用', 'hr', '绿宝瓶', '。'],\n",
       "  ['种', '草', '橘 ', '...'],\n",
       "  ['江湖',\n",
       "   '骗子',\n",
       "   ' ',\n",
       "   '你',\n",
       "   '说',\n",
       "   '的',\n",
       "   '我',\n",
       "   '有',\n",
       "   '用',\n",
       "   '过',\n",
       "   '，',\n",
       "   '橘灿',\n",
       "   '还',\n",
       "   '可以',\n",
       "   '，',\n",
       "   '绿宝瓶',\n",
       "   '2',\n",
       "   '只',\n",
       "   '小样',\n",
       "   '感觉不',\n",
       "   '出来',\n",
       "   '啥',\n",
       "   '，',\n",
       "   '小',\n",
       "   '棕瓶',\n",
       "   '维稳',\n",
       "   '太',\n",
       "   '细水',\n",
       "   '长流',\n",
       "   '了',\n",
       "   '，',\n",
       "   '感觉',\n",
       "   '高能',\n",
       "   '好一点']],\n",
       " 'part_of_speech_tags': [['PN',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'VV',\n",
       "   'VV',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   'PU',\n",
       "   'NR',\n",
       "   'NN',\n",
       "   'NT',\n",
       "   'P',\n",
       "   'VV',\n",
       "   'NN',\n",
       "   'PU',\n",
       "   'NN',\n",
       "   'PU',\n",
       "   'NT',\n",
       "   'VV',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   'PU'],\n",
       "  ['VV', 'NN', 'AD', 'VV', 'PU'],\n",
       "  ['AD', 'VV', 'JJ', 'NN', 'PU'],\n",
       "  ['M', 'NN', 'NN', 'NN'],\n",
       "  ['NN',\n",
       "   'NN',\n",
       "   'NR',\n",
       "   'PN',\n",
       "   'VV',\n",
       "   'DEC',\n",
       "   'PN',\n",
       "   'VE',\n",
       "   'VV',\n",
       "   'AS',\n",
       "   'PU',\n",
       "   'NR',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'PU',\n",
       "   'NN',\n",
       "   'CD',\n",
       "   'AD',\n",
       "   'NN',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'SP',\n",
       "   'PU',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   'VV',\n",
       "   'AD',\n",
       "   'NN',\n",
       "   'VV',\n",
       "   'AS',\n",
       "   'PU',\n",
       "   'VV',\n",
       "   'NN',\n",
       "   'AD']],\n",
       " 'ner_tags': [[('PN AD', 'NT', 0, 2)],\n",
       "  [],\n",
       "  [],\n",
       "  [('NN', 'NT', 3, 4)],\n",
       "  [('NN', 'NT', 0, 1),\n",
       "   ('NR', 'NT', 2, 3),\n",
       "   ('DEC', 'NT', 5, 6),\n",
       "   ('VE', 'NT', 7, 8),\n",
       "   ('VV AS', 'NT', 8, 10),\n",
       "   ('AD', 'NT', 12, 13),\n",
       "   ('SP PU JJ NN VV', 'NT', 21, 26),\n",
       "   ('NN VV AS PU', 'NR', 27, 31),\n",
       "   ('AD', 'NR', 33, 34)]],\n",
       " '_ipython_canary_method_should_not_exist_': []}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(docs[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_pos = {'AD', 'AS', 'BA', 'CC', 'CS','DEC','DEG','DER','DEV','DT', 'ETC',  'LB', 'LC', 'MSP', 'NT', 'OD','ON', 'P','PN','PU','SB','SP','VA','VC','VE','NP','SP'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CD', 'FW', 'IJ', 'JJ', 'M', 'NN', 'NR', 'VP', 'VV', 'X'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tagger.tag_vocab.tokens).difference(useless_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IJ는 ㅋㅋㅋㅋ 같은거임.\n",
    "- 'JJ' + 'NN' JJ는 NN이랑 같이 나올때 의미가 있음.\n",
    "- OD는 서수형\n",
    "- ON은 의성어 (우는게 많다고함.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use_less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['NR', 'NN', 'CC', 'VV', 'NT', 'PU', 'LC', 'AS', 'ETC', 'DEC', 'CD', 'M', 'DEG', 'JJ', 'VC', 'AD', 'P', 'PN', 'VA', 'DEV', 'DT', 'SB', 'OD', 'VE', 'CS', 'MSP', 'BA', 'FW', 'LB', 'DER', 'SP', 'NP', 'IJ', 'X', 'VP'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_vocab.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': ['雪花霜我是回购五六瓶了吧，活动时候黑金卡特惠专用的太难抢了，我从来就没抢到货，到了九点马上付款就没了，只好退而求其次抢了这个套装。'],\n",
       " 'tokens': [['雪花霜',\n",
       "   '我',\n",
       "   '是',\n",
       "   '回购',\n",
       "   '五六',\n",
       "   '瓶',\n",
       "   '了',\n",
       "   '吧',\n",
       "   '，',\n",
       "   '活动',\n",
       "   '时候',\n",
       "   '黑金卡',\n",
       "   '特惠',\n",
       "   '专用',\n",
       "   '的',\n",
       "   '太',\n",
       "   '难',\n",
       "   '抢',\n",
       "   '了',\n",
       "   '，',\n",
       "   '我',\n",
       "   '从来',\n",
       "   '就',\n",
       "   '没',\n",
       "   '抢到',\n",
       "   '货',\n",
       "   '，',\n",
       "   '到',\n",
       "   '了',\n",
       "   '九点',\n",
       "   '马上',\n",
       "   '付款',\n",
       "   '就',\n",
       "   '没',\n",
       "   '了',\n",
       "   '，',\n",
       "   '只好',\n",
       "   '退而求其次',\n",
       "   '抢',\n",
       "   '了',\n",
       "   '这',\n",
       "   '个',\n",
       "   '套装',\n",
       "   '。']],\n",
       " 'part_of_speech_tags': [['NN',\n",
       "   'PN',\n",
       "   'VC',\n",
       "   'VV',\n",
       "   'CD',\n",
       "   'M',\n",
       "   'AS',\n",
       "   'SP',\n",
       "   'PU',\n",
       "   'NN',\n",
       "   'P',\n",
       "   'NR',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   'DEC',\n",
       "   'AD',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'AS',\n",
       "   'PU',\n",
       "   'PN',\n",
       "   'AD',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'VV',\n",
       "   'NN',\n",
       "   'PU',\n",
       "   'VV',\n",
       "   'AS',\n",
       "   'NT',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'AS',\n",
       "   'PU',\n",
       "   'AD',\n",
       "   'VV',\n",
       "   'VV',\n",
       "   'AS',\n",
       "   'DT',\n",
       "   'M',\n",
       "   'NN',\n",
       "   'PU']],\n",
       " 'ner_tags': [[('NN PN VC VV', 'NT', 0, 4),\n",
       "   ('AD', 'NT', 15, 16),\n",
       "   ('VV', 'NT', 17, 18),\n",
       "   ('PU', 'NT', 19, 20),\n",
       "   ('PN', 'NR', 20, 21),\n",
       "   ('AD', 'NT', 21, 22),\n",
       "   ('NN', 'NR', 25, 26),\n",
       "   ('AS', 'NT', 28, 29)]],\n",
       " '_ipython_canary_method_should_not_exist_': []}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"\"\"雪花霜我是回购五六瓶了吧，活动时候黑金卡特惠专用的太难抢了，我从来就没抢到货，到了九点马上付款就没了，只好退而求其次抢了这个套装。\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_path': 'hanlp.components.ner.TransformerNamedEntityRecognizer',\n",
       " 'hanlp_version': '2.0.0-alpha.23',\n",
       " 'load_path': 'https://file.hankcs.com/hanlp/ner/ner_albert_base_zh_msra_20200111_202919.zip'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_capture_config',\n",
       " 'build',\n",
       " 'build_callbacks',\n",
       " 'build_loss',\n",
       " 'build_metrics',\n",
       " 'build_model',\n",
       " 'build_optimizer',\n",
       " 'build_train_dataset',\n",
       " 'build_transform',\n",
       " 'build_valid_dataset',\n",
       " 'build_vocab',\n",
       " 'compile_model',\n",
       " 'config',\n",
       " 'evaluate',\n",
       " 'evaluate_dataset',\n",
       " 'evaluate_output',\n",
       " 'evaluate_output_to_file',\n",
       " 'export_model_for_serving',\n",
       " 'fit',\n",
       " 'from_meta',\n",
       " 'input_shape',\n",
       " 'load',\n",
       " 'load_config',\n",
       " 'load_meta',\n",
       " 'load_transform',\n",
       " 'load_vocabs',\n",
       " 'load_weights',\n",
       " 'meta',\n",
       " 'model',\n",
       " 'on_train_begin',\n",
       " 'predict',\n",
       " 'predict_batch',\n",
       " 'sample_data',\n",
       " 'save',\n",
       " 'save_config',\n",
       " 'save_meta',\n",
       " 'save_vocabs',\n",
       " 'save_weights',\n",
       " 'serve',\n",
       " 'train_loop',\n",
       " 'transform']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ner_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_eagerly': True,\n",
       " 'epochs': 3,\n",
       " 'batch_size': 32,\n",
       " 'metrics': 'f1',\n",
       " 'max_seq_length': 128,\n",
       " 'use_amp': False,\n",
       " 'warmup_steps_ratio': 0,\n",
       " 'clipnorm': 1.0,\n",
       " 'epsilon': 1e-08,\n",
       " 'weight_decay_rate': 0,\n",
       " 'learning_rate': 5e-05,\n",
       " 'optimizer': {'class_name': 'AdamWeightDecay',\n",
       "  'config': {'name': 'AdamWeightDecay',\n",
       "   'clipnorm': 1.0,\n",
       "   'learning_rate': {'class_name': 'PolynomialDecay',\n",
       "    'config': {'initial_learning_rate': 5e-05,\n",
       "     'decay_steps': 3990,\n",
       "     'end_learning_rate': 0.0,\n",
       "     'power': 1.0,\n",
       "     'cycle': False,\n",
       "     'name': None}},\n",
       "   'decay': 0.0,\n",
       "   'beta_1': 0.9,\n",
       "   'beta_2': 0.999,\n",
       "   'epsilon': 1e-08,\n",
       "   'amsgrad': False,\n",
       "   'weight_decay_rate': 0}},\n",
       " 'transformer': 'albert_base_zh',\n",
       " 'warmup_steps': 0,\n",
       " 'train_steps': 3990,\n",
       " 'training': False,\n",
       " 'logger': <Logger hanlp (INFO)>}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d652229ef75f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhanlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'既然句法和语义分析依赖于词性标注'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "hanlp.common.constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = hanlp.load('CTB6_CONVSEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baidu lac \n",
    "- 키워드 추출용 pos set\n",
    "```python\n",
    "{'LOC', 'ORG', 'PER', 'TIME', 'a', 'an', 'm', 'n', 'nr', 'ns', 'nt', 'nz', 'q', 's', 't', 'v', 'vn'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**규칙**\n",
    "- v + 的 u --> 합치기\n",
    "- \"没否不未难\" + (동사 or 형용사) --> 합치기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**키워드 추출기 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAC import LAC\n",
    "\n",
    "lac = LAC(mode='lac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LACKeyPhraseExtractorArguments:\n",
    "    \"\"\"Lac 기반의 KeyPhraseExtractor를 정의하기.\"\"\"\n",
    "    \n",
    "    pos: List[str] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"help\":\"List of Part-of-Speech for extracting keyword candidates\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"use_batch\" : False, # boolean if true\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import unicodedata\n",
    "from typing import List, Union\n",
    "\n",
    "class KeyPhraseExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        args\n",
    "    ):\n",
    "        ## 전처리용 정규식\n",
    "        self.emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "        self.pattern = re.compile(f'[^ .,-/+?!/@$%~％·∼()。、，《 》“”：0-9a-zA-Z\\u4e00-\\u9fff{self.emojis}]+')\n",
    "        self.html = re.compile(\"<(\\\"[^\\\"]*\\\"|'[^']*'|[^'\\\">])*>\")\n",
    "        self.url = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "        self.email = re.compile('([0-9a-zA-Z_]|[^\\s^\\w])+(@)[a-zA-Z]+.[a-zA-Z)]+')\n",
    "        \n",
    "        ## candidates\n",
    "        self.pos = set(args.pos)\n",
    "        \n",
    "    \n",
    "    def _preprocess(self, doc:str) -> str:\n",
    "        # NFKC 정규화\n",
    "        doc = unicodedata.normalize('NFKC', doc)\n",
    "        # html과 JS 제거하기.\n",
    "        if self.html.search(doc) != None: # html js 처리\n",
    "            soup = BeautifulSoup(doc, \"lxml\")\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            doc = soup.get_text()\n",
    "        # 숫자, 문자, 공백, 이모지, 특수문자를 제외한 모든 유니코드 제거\n",
    "        doc = doc.strip()\n",
    "        doc = self.pattern.sub(' ', doc)\n",
    "        # URL을 URL태그로 변환\n",
    "        doc = self.url.sub(' [URL] ', doc) # url\n",
    "        # Email을 email 태그로 변환 \n",
    "        doc = self.email.sub(' [EMAIL] ', doc)\n",
    "        # 해시태그 사이에 spacing\n",
    "        for d in re.findall(r'#(\\w+)', doc):\n",
    "            p = re.compile(f'#{d}')\n",
    "            doc = p.sub(f' #{d} ', doc)\n",
    "            \n",
    "    def _tokenize(self, doc:str) -> List[str]:\n",
    "    \n",
    "    def _postprocess(self, doc):\n",
    "        \"\"\"규칙 기반의 후처리 for lac tokenizer\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def extract(self, doc):\n",
    "        \"\"\"_normalize -> _tokenize -> _postprocess\"\"\"\n",
    "        ##\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "\n",
    "def normalize(doc):\n",
    "    \"\"\"\n",
    "    NFKC normalize\n",
    "    \"\"\"\n",
    "    emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    pattern = re.compile(f'[^ .,-/+?!/@$%~％·∼()。、，《 》“”：0-9a-zA-Z\\u4e00-\\u9fff{emojis}]+')\n",
    "    html = re.compile(\"<(\\\"[^\\\"]*\\\"|'[^']*'|[^'\\\">])*>\")\n",
    "    url = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    email = re.compile('([0-9a-zA-Z_]|[^\\s^\\w])+(@)[a-zA-Z]+.[a-zA-Z)]+')\n",
    "    \n",
    "    \n",
    "    # NFKC 정규화\n",
    "    doc = unicodedata.normalize('NFKC', doc)\n",
    "    # html과 JS 제거하기.\n",
    "    if html.search(doc) != None: # html js 처리\n",
    "        soup = BeautifulSoup(doc, \"lxml\")\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        doc = soup.get_text()\n",
    "    # 숫자, 문자, 공백, 이모지, 특수문자를 제외한 모든 유니코드 제거\n",
    "    doc = doc.strip()\n",
    "    doc = pattern.sub(' ', doc)\n",
    "    # URL을 URL태그로 변환\n",
    "    doc = url.sub(' [URL] ', doc) # url\n",
    "    # Email을 email 태그로 변환 \n",
    "    doc = email.sub(' [EMAIL] ', doc)\n",
    "    # 해시태그 사이에 spacing\n",
    "    for d in re.findall(r'#(\\w+)', doc):\n",
    "        p = re.compile(f'#{d}')\n",
    "        doc = p.sub(f' #{d} ', doc)\n",
    "        \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAC import LAC\n",
    "\n",
    "lac = LAC(mode='lac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"还没收到货，不知道怎么样\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 제품명 하나를 통째로 인식하는것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "还 d\n",
      "没 v\n",
      "收到 v\n",
      "货 n\n",
      "， w\n",
      "不知道 v\n",
      "怎么样 r\n"
     ]
    }
   ],
   "source": [
    "for tok, p in zip(*lac.run(data)):\n",
    "    print(tok, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hanlp\n",
    "- 장점\n",
    "- 단점\n",
    "\n",
    "lac\n",
    "- 장점\n",
    "- 단점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['兰蔻新清滢柔肤水(粉水)400ml惠选套组 这次买的,没上次买的好,无论瓶子,瓶盖,之类的都比上次的差,粉水也没上次浓,香']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['sentences']#['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dataclasses\n",
    "from dataclasses import dataclass,field\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class InputText:\n",
    "    \"\"\"\n",
    "    A single Document data for Text Data analysis.\n",
    "    \n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        title: (Optional) string. The raw text for document's title.\n",
    "        content: string. The raw text for document's contents.\n",
    "    \"\"\"\n",
    "\n",
    "    guid: str\n",
    "    content: str\n",
    "    title: Optional[str] = None\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = InputText(guid = 1, content='yabal', title='yobol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.content = unicodedata.normalize('NFKC', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "normalize() argument 2 must be str, not None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-211000ab5545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFKC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: normalize() argument 2 must be str, not None"
     ]
    }
   ],
   "source": [
    "unicodedata.normalize('NFKC', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.List[typing.List[str]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List[List[str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "if 'd':\n",
    "    print('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dataclasses\n",
    "from dataclasses import dataclass,field\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class InputText:\n",
    "    \"\"\"\n",
    "    A single Document data for Text Data analysis.\n",
    "    \n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        title: (Optional) string. The raw text for document's title.\n",
    "        content: string. The raw text for document's contents.\n",
    "    \"\"\"\n",
    "\n",
    "    guid: str\n",
    "    title: Optional[str] = None\n",
    "    content: str = None\n",
    "    \n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TextFeatures:\n",
    "    \"\"\"\n",
    "    The textual features of single Document.\n",
    "    \n",
    "    Args:\n",
    "        {title, content}_tokens: List of String. Tokenized token list for raw text.\n",
    "        {title, content}_pos: (Optional) List of String. Part-of-speeches for each token.\n",
    "    \"\"\"\n",
    "    \n",
    "    title_tokens: List[str]\n",
    "    content_tokens: List[str]\n",
    "    \n",
    "    title_pos: Optional[List[str]] = []\n",
    "    content_pos: Optional[List[str]] = []\n",
    "    #### more features with the development\n",
    "    \n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self)) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextFeatures(title_tokens=['안녕'], content_tokens=['하세요'], title_pos=None, content_pos=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextFeatures(title_tokens=['안녕'], content_tokens=['하세요'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.InputText"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextFeatures.content_pos = ['그래서 뭐?']\n",
    "\n",
    "TextFeatures.content_pos.append('어쩌라구')\n",
    "\n",
    "TextFeatures.content_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.Union[str, NoneType]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Union[str, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('쿠팡', '에서', '후기', '가', '좋길래', '닦토용', '으로', '샀는데', '별로에요', '....'),\n",
       " ('L', 'R', 'L', 'R', 'L', 'L', 'R', 'L', 'L', 'R')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*[('쿠팡', 'L'), ('에서', 'R'), ('후기', 'L'), ('가', 'R'), ('좋길래', 'L'), ('닦토용', 'L'), ('으로', 'R'), ('샀는데', 'L'), ('별로에요', 'L'), ('....', 'R')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    return ['안녕'], ['디지몬']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['안녕'], ['디지몬'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "兰蔻新清滢 nz\n",
      "柔肤水 nz\n",
      "( w\n",
      "粉 n\n",
      "水 n\n",
      ") w\n",
      "400ml惠选套组 m\n",
      "  w\n",
      "这次 r\n",
      "买 v\n",
      "的 u\n",
      ", w\n",
      "没 v\n",
      "上次 r\n",
      "买 v\n",
      "的 u\n",
      "好 a\n",
      ", w\n",
      "无论 c\n",
      "瓶子 n\n",
      ", w\n",
      "瓶盖 n\n",
      ", w\n",
      "之类 r\n",
      "的 u\n",
      "都 d\n",
      "比 p\n",
      "上次 r\n",
      "的 u\n",
      "差 n\n",
      ", w\n",
      "粉 n\n",
      "水 n\n",
      "也 d\n",
      "没 v\n",
      "上次 n\n",
      "浓 a\n",
      ", w\n",
      "香 n\n"
     ]
    }
   ],
   "source": [
    "for tok, p in zip(*lac.run(tmp['sentences'][0])):\n",
    "    print(tok, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "with open('sample.txt') as f:\n",
    "    docs = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set = {'LOC', 'ORG', 'PER', 'TIME', 'a', 'an', 'm', 'n', 'nr', 'ns', 'nt', 'nz', 'q', 's', 't', 'v', 'vn'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAC import LAC\n",
    "\n",
    "lac = LAC(mode='lac', use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "EnforceNotMet",
     "evalue": "\n\n--------------------------------------------\nC++ Call Stacks (More useful to developers):\n--------------------------------------------\n0   std::string paddle::platform::GetTraceBackString<char const*>(char const*&&, char const*, int)\n1   paddle::platform::EnforceNotMet::EnforceNotMet(std::__exception_ptr::exception_ptr, char const*, int)\n2   paddle::platform::dynload::EnforceCUDNNLoaded(char const*)\n3   paddle::platform::CUDADeviceContext::CUDADeviceContext(paddle::platform::CUDAPlace)\n4   std::_Function_handler<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > (), std::reference_wrapper<std::_Bind_simple<paddle::platform::EmplaceDeviceContext<paddle::platform::CUDADeviceContext, paddle::platform::CUDAPlace>(std::map<paddle::platform::Place, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >, std::less<paddle::platform::Place>, std::allocator<std::pair<paddle::platform::Place const, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > > > > >*, paddle::platform::Place)::{lambda()#1} ()> > >::_M_invoke(std::_Any_data const&)\n5   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >, std::__future_base::_Result_base::_Deleter>, std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > > >::_M_invoke(std::_Any_data const&)\n6   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)\n7   std::__future_base::_Deferred_state<std::_Bind_simple<paddle::platform::EmplaceDeviceContext<paddle::platform::CUDADeviceContext, paddle::platform::CUDAPlace>(std::map<paddle::platform::Place, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >, std::less<paddle::platform::Place>, std::allocator<std::pair<paddle::platform::Place const, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > > > > >*, paddle::platform::Place)::{lambda()#1} ()>, std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >::_M_run_deferred()\n8   paddle::platform::DeviceContextPool::Get(paddle::platform::Place const&)\n9   paddle::PaddleTensorToLoDTensor(paddle::PaddleTensor const&, paddle::framework::LoDTensor*, paddle::platform::Place const&)\n10  paddle::AnalysisPredictor::SetFeed(std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> > const&, paddle::framework::Scope*)\n11  paddle::AnalysisPredictor::Run(std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> > const&, std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> >*, int)\n\n----------------------\nError Message Summary:\n----------------------\nError: Cannot load cudnn shared library. Cannot invoke method cudnnGetVersion at (/paddle/paddle/fluid/platform/dynload/cudnn.cc:63)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mEnforceNotMet\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8fcda52c7b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/LAC/lac.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mtensor_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mcrf_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrf_decode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEnforceNotMet\u001b[0m: \n\n--------------------------------------------\nC++ Call Stacks (More useful to developers):\n--------------------------------------------\n0   std::string paddle::platform::GetTraceBackString<char const*>(char const*&&, char const*, int)\n1   paddle::platform::EnforceNotMet::EnforceNotMet(std::__exception_ptr::exception_ptr, char const*, int)\n2   paddle::platform::dynload::EnforceCUDNNLoaded(char const*)\n3   paddle::platform::CUDADeviceContext::CUDADeviceContext(paddle::platform::CUDAPlace)\n4   std::_Function_handler<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > (), std::reference_wrapper<std::_Bind_simple<paddle::platform::EmplaceDeviceContext<paddle::platform::CUDADeviceContext, paddle::platform::CUDAPlace>(std::map<paddle::platform::Place, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >, std::less<paddle::platform::Place>, std::allocator<std::pair<paddle::platform::Place const, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > > > > >*, paddle::platform::Place)::{lambda()#1} ()> > >::_M_invoke(std::_Any_data const&)\n5   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >, std::__future_base::_Result_base::_Deleter>, std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > > >::_M_invoke(std::_Any_data const&)\n6   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)\n7   std::__future_base::_Deferred_state<std::_Bind_simple<paddle::platform::EmplaceDeviceContext<paddle::platform::CUDADeviceContext, paddle::platform::CUDAPlace>(std::map<paddle::platform::Place, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >, std::less<paddle::platform::Place>, std::allocator<std::pair<paddle::platform::Place const, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > > > > >*, paddle::platform::Place)::{lambda()#1} ()>, std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >::_M_run_deferred()\n8   paddle::platform::DeviceContextPool::Get(paddle::platform::Place const&)\n9   paddle::PaddleTensorToLoDTensor(paddle::PaddleTensor const&, paddle::framework::LoDTensor*, paddle::platform::Place const&)\n10  paddle::AnalysisPredictor::SetFeed(std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> > const&, paddle::framework::Scope*)\n11  paddle::AnalysisPredictor::Run(std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> > const&, std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> >*, int)\n\n----------------------\nError Message Summary:\n----------------------\nError: Cannot load cudnn shared library. Cannot invoke method cudnnGetVersion at (/paddle/paddle/fluid/platform/dynload/cudnn.cc:63)\n"
     ]
    }
   ],
   "source": [
    "lac.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lac.txt','w') as f:\n",
    "    for doc in docs:\n",
    "        doc = normalize(doc) \n",
    "        print(\" \".join([token for token, pos in zip(*lac.run(doc)) if pos in pos_set]), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT-KPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dataset/openkp/OpenKPDev.jsonl') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = open('data/dataset/openkp/OpenKPDev.jsonl')\n",
    "result = [json.loads(jline) for jline in response.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', 'text', 'VDOM', 'KeyPhrases'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Home of the Top Heavy Pizza Sandwiches Welcome 54 Pizza Express offers a high quality homemade product served with friendly efficient service for Dinein Carry out or Delivery Every 54 Pizza Express pizza is a quality pizza made with wholesome quality ingredients fresh vegetables delectable meats rich tangy tomato sauce and creamy mozzarella cheese Made fresh daily and served with your choice of dressing Choose from a Side Salad Dinner Salad Chef Salad Griled Chicken Salad Tuna Salad Buffalo Grilled Chicken Salad or Pepperoni and Bacon Salad Hot Toasted Strombolis Lasagna Baked Spaghetti Breadsticks stuffed with Mozzarella or build your own breadsticks Hot toasted Sub Sandwiches Calzones or Wings Check out the menu for our Homemade Desserts Straight from our oven to you We have two loactions to choose from 1700 Starlite Drive 2706835400 3101 Alvey Park Drive 2706835454 Quality Passion Pride We offer a high quality homemade product served with friendly efficient service for Dinein Carry out or Delivery Every 54 Pizza Express pizza is a quality pizza made with wholesome quality ingredient fresh vegetables delectable meats rich tangy tomato sauce and creamy mozzarella cheese At 54 Pizza Express we provide quality you can afford and a taste thats unforgettable Cindy Tong'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['54', 'Pizza', 'Express'],\n",
       " ['wholesome', 'quality', 'ingredient'],\n",
       " ['homemade', 'product']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]['KeyPhrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Utilities Administration Administrative Services Meet the Director 311 Project HOPE Program RoundUp About Us Albany Utilities Utilities Administration Administrative Services The Administrative Services Divisions primary function is to provide TERRIFIC customer service to the citizens of Albany and surrounding areas We pride ourselves on giving the citizens of Albany the best possible service We handle all utility needs from establishing commercial and residential new services transferring existing services making payments performing energy conservation audits implementing weatherization projects and rereading or auditing utility services We also process all utility bill complaints and concerns Our 311 Customer Service Call Center handles all nonemergency calls for the City of Albany and Dougherty County Our Terrific Values TRUST Do what we say we will do EFFECTIVENESS Perform all functions with excellence RESPONSIVENESS Exhibit a sense of urgency we value time and use it well RESPECT Treat people well we value the opinions of others INTEGRITY We will do the right thing FUN No day is complete without laughter INNOVATION Look for and embrace new effective ways to do things CUSTOMER SATISFACTION Delight citizens everyday through exemplary service Click Here to View your Utility Bill Click Here to Pay your Utility Bill'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[10]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Administrative', 'Services'],\n",
       " ['City', 'of', 'Albany'],\n",
       " ['Dougherty', 'County']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[10]['KeyPhrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"url\": \"http://1hackerstock.blogspot.com/2016/09/nba-2k17-cd-key-generator-no-survey-pc.html\", \"text\": \"Home Keygen NBA 2K17 nba 2k17 activation key generator nba 2k17 beta keygen free nba 2k17 cd codes free nba 2k17 cd download nba 2k17 cd generator no survey nba 2k17 cd key download without survey NBA 2K17 CD Key Generator No Survey PC PS34 Xbox 360ONE NBA 2K17 CD Key Generator No Survey PC PS34 Xbox 360ONE Penulis Hacker Stock on Friday 9 September 2016 1253 NBA 2K17 CD Key Generator No Survey PC PS34 Xbox 360ONE Hello everybody welcome on our web site HackerStockcom these days weve a replacement Key Generator for you that is known as NBA 2K17 CD Key Generator No Survey PC PS34 Xbox 360ONE youll be ready to get the game without charge this keygen will find unlimited Activation Codes for you on any platform Steam or Origin on computer or why not PlayStation and Xbox Weve ready one thing special for all NBA fans and players a special tool that were certain that you just will agree Our tool may generate tons of key codes for laptop PlayStation 3 PlayStation 4 Xbox 360 and Xbox ONE So youll get early access to the current game through our key generator for NBA 2K17 simply with few clicks This tool will generate over 800 000 key codes for various platforms The key code is valid and youll be ready to try it and be able to play NBA 2K17 without charge Our serial key generator tool is NBA 2K17 CD Key Generator No Survey PC PS34 Xbox 360ONE Instructions using the NBA 2K17 CD Key Generator 2017 is quick and easy First just download the exe file and install it on your computer After running the program select the platform on which you want to play NBA 2K17 Next click the GENERATE button This will produce an alphanumeric code also known as your product key You will use it to validate the authenticity of your NBA 2K17 game Now copy and paste the product key onto the serial number window prompt of your NBA 2K17 software You will gain access to NBA 2K17 Finally enjoy your game We designed this NBA 2K17 game key generator to the best of our abilities We truly hope that you take advantage of its features to fully enjoy your NBA 2K17 Please let us know if you encounter any problems with our software We would love to help you out NBA 2K17 nba 2k17 activation key generator nba 2k17 beta keygen free nba 2k17 cd codes free nba 2k17 cd download nba 2k17 cd generator no survey nba 2k17 cd key download without survey nba 2k17 cd key free download nba 2k17 cd key free online nba 2k17 cd key generator no survey nba 2k17 cd key pc download nba 2k17 cd key ps4 free download nba 2k17 cd key xbox free download nba 2k17 cd keyexe no survey nba 2k17 crack version download nba 2k17 download 2016 nba 2k17 download for pc 2016 nba 2k17 download full crack nba Posted by Hacker Stock at 1253 Email This BlogThis Labels Keygen NBA 2K17 nba 2k17 activation key generator nba 2k17 beta keygen free nba 2k17 cd codes free nba 2k17 cd download nba 2k17 cd generator no survey nba 2k17 cd key download without survey Older Post Home Subscribe to Post Comments Atom\", \"VDOM\": \"[{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Home\\\\\",\\\\\"feature\\\\\":[32.0,34.0,199.0,15.0,0.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":0,\\\\\"end_idx\\\\\":1},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Keygen\\\\\",\\\\\"feature\\\\\":[79.0,41.0,199.0,15.0,0.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":1,\\\\\"end_idx\\\\\":2},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"NBA 2K17\\\\\",\\\\\"feature\\\\\":[129.0,56.0,199.0,15.0,0.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":2,\\\\\"end_idx\\\\\":4},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"nba 2k17 activation key generator\\\\\",\\\\\"feature\\\\\":[194.0,182.0,199.0,15.0,0.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":4,\\\\\"end_idx\\\\\":9},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"nba 2k17 beta keygen free\\\\\",\\\\\"feature\\\\\":[385.0,144.0,199.0,15.0,0.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":9,\\\\\"end_idx\\\\\":14},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"nba 2k17 cd codes free\\\\\",\\\\\"feature\\\\\":[32.0,573.0,199.0,31.0,0.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":14,\\\\\"end_idx\\\\\":19},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"nba 2k17 cd download\\\\\",\\\\\"feature\\\\\":[99.0,124.0,215.0,15.0,0.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":19,\\\\\"end_idx\\\\\":23},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"nba 2k17 cd generator no survey\\\\\",\\\\\"feature\\\\\":[232.0,178.0,215.0,15.0,0.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":23,\\\\\"end_idx\\\\\":29},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"nba 2k17 cd key download without survey\\\\\",\\\\\"feature\\\\\":[32.0,574.0,215.0,31.0,0.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":29,\\\\\"end_idx\\\\\":36},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"NBA 2K17 CD Key Generator No Survey PC PS34 Xbox 360ONE\\\\\",\\\\\"feature\\\\\":[32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,194.0,62.0,1.0,0.0,0.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":36,\\\\\"end_idx\\\\\":47},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"NBA 2K17 CD Key Generator No Survey PC PS34 Xbox 360ONE\\\\\",\\\\\"feature\\\\\":[32.0,610.0,261.0,58.0,1.0,0.0,1.0,0.0,23.0,0.0,32.0,610.0,261.0,58.0,1.0,0.0,1.0,0.0,23.0,0.0],\\\\\"start_idx\\\\\":47,\\\\\"end_idx\\\\\":58},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Penulis Hacker Stock on Friday 9 September 2016 1253\\\\\",\\\\\"feature\\\\\":[32.0,331.0,320.0,15.0,0.0,0.0,1.0,0.0,12.0,0.0,32.0,610.0,320.0,14.0,1.0,0.0,1.0,0.0,12.0,0.0],\\\\\"start_idx\\\\\":58,\\\\\"end_idx\\\\\":67},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"NBA 2K17 CD Key Generator No Survey PC PS34 Xbox 360ONE\\\\\",\\\\\"feature\\\\\":[34.0,606.0,363.0,60.0,0.0,0.0,1.0,0.0,25.0,0.0,32.0,610.0,362.0,66.0,1.0,0.0,1.0,0.0,25.0,0.0],\\\\\"start_idx\\\\\":67,\\\\\"end_idx\\\\\":78},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Hello everybody welcome on our web site HackerStockcom these days weve a replacement Key Generator for you that is known as NBA 2K17 CD Key Generator No Survey PC PS34 Xbox 360ONE youll be ready to get the game without charge this keygen will find unlimited Activation Codes for you on any platform Steam or Origin on computer or why not PlayStation and Xbox\\\\\",\\\\\"feature\\\\\":[32.0,610.0,362.0,1842.0,1.0,0.0,0.0,1.0,25.0,0.0,32.0,610.0,362.0,1842.0,1.0,0.0,0.0,1.0,25.0,0.0],\\\\\"start_idx\\\\\":78,\\\\\"end_idx\\\\\":142},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Weve ready one thing special for all NBA fans and players a special tool that were certain that you just will agree Our tool may generate tons of key codes for laptop PlayStation 3 PlayStation 4 Xbox 360 and Xbox ONE So youll get early access to the current game through our key generator for NBA 2K17 simply with few clicks This tool will generate over 800 000 key codes for various platforms The key code is valid and youll be ready to try it and be able to play NBA 2K17 without charge Our serial key generator tool is\\\\\",\\\\\"feature\\\\\":[32.0,610.0,362.0,1842.0,1.0,0.0,0.0,1.0,25.0,0.0,32.0,610.0,362.0,1842.0,1.0,0.0,0.0,1.0,25.0,0.0],\\\\\"start_idx\\\\\":142,\\\\\"end_idx\\\\\":242},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"NBA 2K17 CD Key Generator No Survey PC PS34 Xbox 360ONE\\\\\",\\\\\"feature\\\\\":[111.0,452.0,959.0,16.0,0.0,0.0,1.0,0.0,14.0,0.0,32.0,610.0,958.0,18.0,1.0,0.0,1.0,0.0,14.0,0.0],\\\\\"start_idx\\\\\":242,\\\\\"end_idx\\\\\":253},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Instructions using the NBA 2K17 CD Key Generator 2017 is quick and easy\\\\\",\\\\\"feature\\\\\":[32.0,503.0,1257.0,16.0,0.0,0.0,1.0,0.0,14.0,0.0,32.0,610.0,1256.0,18.0,1.0,0.0,1.0,0.0,14.0,0.0],\\\\\"start_idx\\\\\":253,\\\\\"end_idx\\\\\":266},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"First just download the exe file and install it on your computer\\\\\",\\\\\"feature\\\\\":[72.0,570.0,1292.0,18.0,1.0,0.0,0.0,0.0,14.0,0.0,72.0,570.0,1292.0,18.0,1.0,0.0,0.0,0.0,14.0,0.0],\\\\\"start_idx\\\\\":266,\\\\\"end_idx\\\\\":278},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"After running the program select the platform on which you want to play NBA 2K17\\\\\",\\\\\"feature\\\\\":[72.0,570.0,1310.0,18.0,1.0,0.0,0.0,0.0,14.0,0.0,72.0,570.0,1310.0,18.0,1.0,0.0,0.0,0.0,14.0,0.0],\\\\\"start_idx\\\\\":278,\\\\\"end_idx\\\\\":293},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Next click the GENERATE button This will produce an alphanumeric code also known as your product key You will use it to validate the authenticity of your NBA 2K17 game\\\\\",\\\\\"feature\\\\\":[72.0,570.0,1328.0,36.0,1.0,0.0,0.0,0.0,14.0,0.0,72.0,570.0,1328.0,36.0,1.0,0.0,0.0,0.0,14.0,0.0],\\\\\"start_idx\\\\\":293,\\\\\"end_idx\\\\\":323},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Now copy and paste the product key onto the serial number window prompt of your NBA 2K17 software You will gain access to NBA 2K17\\\\\",\\\\\"feature\\\\\":[72.0,570.0,1364.0,36.0,1.0,0.0,0.0,0.0,14.0,0.0,72.0,570.0,1364.0,36.0,1.0,0.0,0.0,0.0,14.0,0.0],\\\\\"start_idx\\\\\":323,\\\\\"end_idx\\\\\":348},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Finally enjoy your game\\\\\",\\\\\"feature\\\\\":[72.0,570.0,1400.0,18.0,1.0,0.0,0.0,0.0,14.0,0.0,72.0,570.0,1400.0,18.0,1.0,0.0,0.0,0.0,14.0,0.0],\\\\\"start_idx\\\\\":348,\\\\\"end_idx\\\\\":352},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"We designed this NBA 2K17 game key generator to the best of our abilities We truly hope that you take advantage of its features to fully enjoy your NBA 2K17 Please let us know if you encounter any problems with our software We would love to help you out\\\\\",\\\\\"feature\\\\\":[72.0,570.0,1418.0,54.0,1.0,0.0,0.0,0.0,14.0,0.0,72.0,570.0,1418.0,54.0,1.0,0.0,0.0,0.0,14.0,0.0],\\\\\"start_idx\\\\\":352,\\\\\"end_idx\\\\\":401},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"NBA 2K17 nba 2k17 activation key generator nba 2k17 beta keygen free nba 2k17 cd codes free nba 2k17 cd download nba 2k17 cd generator no survey nba 2k17 cd key download without survey nba 2k17 cd key free download nba 2k17 cd key free online nba 2k17 cd key generator no survey nba 2k17 cd key pc download nba 2k17 cd key ps4 free download nba 2k17 cd key xbox free download nba 2k17 cd keyexe no survey nba 2k17 crack version download nba 2k17 download 2016 nba 2k17 download for pc 2016 nba 2k17 download full crack nba\\\\\",\\\\\"feature\\\\\":[32.0,610.0,362.0,1842.0,1.0,0.0,0.0,1.0,25.0,0.0,32.0,610.0,362.0,1842.0,1.0,0.0,0.0,1.0,25.0,0.0],\\\\\"start_idx\\\\\":401,\\\\\"end_idx\\\\\":501},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Posted by\\\\\",\\\\\"feature\\\\\":[32.0,135.0,2517.0,15.0,0.0,0.0,0.0,0.0,12.0,0.0,32.0,610.0,2455.0,142.0,1.0,0.0,0.0,1.0,12.0,0.0],\\\\\"start_idx\\\\\":501,\\\\\"end_idx\\\\\":503},{\\\\\"Id\\\\\":0,\\\\\"text\\\\\":\\\\\"Hacker Stock\\\\\",\\\\\"feature\\\\\":[90.0,74.0,2517.0,15.0,0.0,0.0,0.0,0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ap-ngsd-sb-ko.py 에러 검증.\n",
    "    - 개선작업\n",
    "- keyphraser\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyPhraser:\n",
    "    def __init__(\n",
    "        self,\n",
    "        args\n",
    "    ):\n",
    "        pass\n",
    "    \n",
    "    def _preprocess(self, doc):\n",
    "        pass\n",
    "    \n",
    "    def _postprocess(self, doc):\n",
    "        pass\n",
    "    \n",
    "    def extract(self, doc):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hanlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
